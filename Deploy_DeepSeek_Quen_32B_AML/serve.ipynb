{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "credential = DefaultAzureCredential()\n",
        "workspace_ml_client = None\n",
        "try:\n",
        "    workspace_ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\nOverriding of current TracerProvider is not allowed\nOverriding of current LoggerProvider is not allowed\nOverriding of current MeterProvider is not allowed\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1738058738840
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subscription_id = workspace_ml_client.subscription_id\n",
        "resource_group = workspace_ml_client.resource_group_name\n",
        "workspace_name = workspace_ml_client.workspace_name\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "endpoint_name = \"deepseek-quen-15B-endpoint\""
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1738072721278
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"model\"\n",
        "import os\n",
        "os.makedirs(base_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1738058677311
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\n",
        "            repo_id=model_name,\n",
        "            local_dir=model_path,\n",
        "            #ignore_patterns=[\"*.safetensors\", \"*.bin\"]\n",
        "        )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nFetching 9 files: 100%|██████████| 9/9 [01:38<00:00, 10.95s/it]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/mrajguru3/code/Users/mrajguru/deepseek/model'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1738058672420
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "'model'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1738059002094
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model, Environment\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import os\n",
        "from uuid import uuid4\n",
        "local_model = Model(\n",
        "    path=model_path,\n",
        "    type=AssetTypes.CUSTOM_MODEL,\n",
        "    name=\"r1-distil-quen-15b\",\n",
        "    version=\"1\",\n",
        "    description=\"Model created from local file for text generation deployment.\",\n",
        ")\n",
        "model = workspace_ml_client.models.create_or_update(local_model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/mrajguru3/code/Users/mrajguru/deepseek/model' 'https://amldevsweden3845944479.blob.core.windows.net/azureml-blobstore-06e6f83f-87ce-4aa8-8df6-8e5cda0ac300/LocalUpload/00cca47d13f6a3b1f8b424964570907a/model' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1738059394202
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "env_docker_image = Environment(\n",
        "            build=BuildContext(path=\"environment\"),\n",
        "            name=\"vllm-custom\",\n",
        "            description=\"Environment created from a Docker context.\",\n",
        "            inference_config={\n",
        "                \"liveness_route\": {\n",
        "                    \"port\": 8000,\n",
        "                    \"path\": \"/health\",\n",
        "                },\n",
        "                \"readiness_route\": {\n",
        "                    \"port\": 8000,\n",
        "                    \"path\": \"/health\",\n",
        "                },\n",
        "                \"scoring_route\": {\n",
        "                    \"port\": 8000,\n",
        "                    \"path\": \"/\",\n",
        "                },\n",
        "            },\n",
        "        )\n",
        "env_asset = workspace_ml_client.environments.create_or_update(env_docker_image)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading environment (0.0 MBs): 100%|██████████| 612/612 [00:00<00:00, 3368.01it/s]\n\u001b[39m\n\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1738059685055
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    IdentityConfiguration,\n",
        "    ManagedIdentityConfiguration,\n",
        ")\n",
        "\n",
        "# Check if the endpoint already exists in the workspace\n",
        "try:\n",
        "    endpoint = workspace_ml_client.online_endpoints.get(endpoint_name)\n",
        "except:\n",
        "    endpoint = ManagedOnlineEndpoint(\n",
        "        name=endpoint_name,\n",
        "        description=f\"Test endpoint for {model.name}\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1738072725913
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_ml_client.begin_create_or_update(endpoint).wait()"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1738072791772
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_vars = {\n",
        "    \"MODEL_NAME\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "    \"VLLM_ARGS\": \"--max-model-len 16000 --enforce-eager\",\n",
        "}\n",
        "deployment_env_vars = {**env_vars}"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1738083809850
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from azure.ai.ml.entities import (    \n",
        "    OnlineRequestSettings,\n",
        "    CodeConfiguration,\n",
        "    ManagedOnlineDeployment,\n",
        "    ProbeSettings,\n",
        "    Environment\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=  \"deepseek-quen-15b-deployment\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    #model=model,\n",
        "    instance_type=\"Standard_NC24ads_A100_v4\",\n",
        "    instance_count=1,\n",
        "    #code_configuration=code_configuration,\n",
        "    environment_variables=deployment_env_vars,    \n",
        "    environment=env_asset,\n",
        "    # scoring_script=\"score.py\",\n",
        "    # code_path=\"./src_serve\",\n",
        "    request_settings=OnlineRequestSettings(\n",
        "        max_concurrent_requests_per_instance=2,\n",
        "        request_timeout_ms=50000, \n",
        "        max_queue_wait_ms=60000\n",
        "    ),\n",
        "    liveness_probe=ProbeSettings(\n",
        "        failure_threshold=5,\n",
        "        success_threshold=1,\n",
        "        timeout=10,\n",
        "        period=30,\n",
        "        initial_delay=120\n",
        "    ),\n",
        "    readiness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        timeout=2,\n",
        "        period=10,\n",
        "        initial_delay=120,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Trigger the deployment creation\n",
        "try:\n",
        "    workspace_ml_client.begin_create_or_update(deployment).wait()\n",
        "except Exception as err:\n",
        "    raise RuntimeError(\n",
        "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
        "    ) from err\n",
        "    \n",
        "endpoint.traffic = {\"deepseek-quen-15b-deployment\": 100}\n",
        "endpoint_poller = workspace_ml_client.online_endpoints.begin_create_or_update(endpoint)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint deepseek-quen-15b-endpoint exists\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "............."
        }
      ],
      "execution_count": 72,
      "metadata": {
        "gather": {
          "logged": 1738084358555
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint_results = endpoint_poller.result()\n",
        "endpoint_name = endpoint_results.name\n",
        "keys = workspace_ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
        "primary_key = keys.primary_key"
      ],
      "outputs": [],
      "execution_count": 73,
      "metadata": {
        "gather": {
          "logged": 1738122572091
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "url = os.path.join(endpoint_results.scoring_uri, \"v1\")\n",
        "endpoint_name = (\n",
        "    endpoint_results.name if endpoint_name is None else endpoint_name\n",
        ")\n",
        "keys = workspace_ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
        "vllm_client = OpenAI(base_url=url, api_key=primary_key)\n",
        "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\""
      ],
      "outputs": [],
      "execution_count": 74,
      "metadata": {
        "gather": {
          "logged": 1738122575226
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your prompt\n",
        "system_message = \"\"\"You are a helpful Assistant\"\"\"\n",
        "user_message = f\"\"\"write a python code to read a csv\"\"\"\n",
        "response = vllm_client .chat.completions.create(\n",
        "    model=model_path,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=4000,\n",
        "    stream=True,  # Stream the response\n",
        ")\n",
        " \n",
        "print(\"Streaming response:\")\n",
        "for chunk in response:\n",
        "    delta = chunk.choices[0].delta\n",
        "    if hasattr(delta, \"content\"):\n",
        "        print(delta.content, end=\"\", flush=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Streaming response:\n<think>\nOkay, the user wants me to write a Python code to read a CSV file. I remember that Python has a built-in module called csv which is good for handling CSV files. But wait, I've also heard that pandas is a library that makes working with data easier. Maybe I should provide both methods so the user can choose depending on their needs.\n\nFirst, I'll start with the standard library approach using csv. I'll need to import the csv module. Then, I'll open the file using a with statement to ensure it's properly closed after reading. I'll use csv.reader to read the file, and probably read the header first so the user can see the columns. Then, I'll loop through each row and print it out.\n\nNext, for the pandas method, I'll need to import pandas as pd. Then, I can read the CSV file using pd.read_csv and store it in a DataFrame. I should print out the first few rows using head() so the user can see the data quickly. Also, maybe include some comments about installing pandas if they haven't already.\n\nI should make sure the code is clear and well-commented. Maybe ask the user if they need any modifications or further details. I'll structure the answer by first explaining both methods, then provide the code with explanations so the user understands what each part does.\n</think>\n\nHere's a Python code to read a CSV file using the `csv` module:\n\n```python\nimport csv\n\n# Open the CSV file\nwith open('your_file.csv', 'r') as file:\n    # Create a CSV reader\n    reader = csv.reader(file)\n    \n    # Read the header\n    header = next(reader)\n    print(\"Header:\", header)\n    \n    # Read each row\n    for row in reader:\n        print(\"Row:\", row)\n```\n\nIf you want to use the `pandas` library for more advanced data handling, you can do:\n\n```python\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('your_file.csv')\n\n# Display the first few rows\nprint(df.head())\n\n# Display the column names\nprint(\"Columns:\", df.columns)\n```\n\nMake sure to replace `'your_file.csv'` with the actual path to your CSV file. For the `pandas` method, you'll need"
        }
      ],
      "execution_count": 82,
      "metadata": {
        "gather": {
          "logged": 1738144492165
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}