{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e43ce9-c243-4046-8695-9c10f75f6a91",
   "metadata": {},
   "source": [
    "### Loading the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbc525e8-3db2-4182-93b0-c401863af600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:16:42.757293Z",
     "iopub.status.busy": "2024-07-19T13:16:42.756773Z",
     "iopub.status.idle": "2024-07-19T13:16:42.774137Z",
     "shell.execute_reply": "2024-07-19T13:16:42.771557Z",
     "shell.execute_reply.started": "2024-07-19T13:16:42.757293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('azure.env',override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d3650-546a-4b4f-99b1-7151fd429e3f",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6e2f82-97c8-48f6-98a8-abe25c0e30a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:05:54.808465Z",
     "iopub.status.busy": "2024-07-19T13:05:54.807389Z",
     "iopub.status.idle": "2024-07-19T13:06:05.785539Z",
     "shell.execute_reply": "2024-07-19T13:06:05.782980Z",
     "shell.execute_reply.started": "2024-07-19T13:05:54.808465Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings,AzureChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import PostgresChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "from doc_intelligence import AzureAIDocumentIntelligenceLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd5389-4c40-463a-893a-c331525e7e33",
   "metadata": {},
   "source": [
    "### Uploading the document through Azure Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9771883b-0548-4db9-86b6-2ae107dce3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:06:05.793467Z",
     "iopub.status.busy": "2024-07-19T13:06:05.790800Z",
     "iopub.status.idle": "2024-07-19T13:08:25.929099Z",
     "shell.execute_reply": "2024-07-19T13:08:25.926945Z",
     "shell.execute_reply.started": "2024-07-19T13:06:05.792926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures:\n",
      "Figure #0 has the following spans: [{'offset': 3122, 'length': 445}]\n",
      "Span #0: {'offset': 3122, 'length': 445}\n",
      "Original figure content in markdown: \n",
      "<figure>\n",
      "\n",
      "![](figures/0)\n",
      "\n",
      "<!-- FigureContent=\"Papers Released over Years LLMs 20900 LLMs + Fine-Tuning 7260 LLMs + Alignment 4740 1210 582 Papers 260 238 153 114 60 56 58 42 32 26 17 12 3 2018 2019 2020 2021 2022 2023 Year\" -->\n",
      "\n",
      "<figcaption>\n",
      "\n",
      "Figure 1: The trend of papers released over years containing keywords \"Large\n",
      "Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large Lan-\n",
      "guage Model + Alignment\".\n",
      "\n",
      "</figcaption>\n",
      "\n",
      "</figure>\n",
      "\n",
      "\n",
      "\tCaption: Figure 1: The trend of papers released over years containing keywords \"Large\n",
      "Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large Lan-\n",
      "guage Model + Alignment\".\n",
      "\tCaption bounding region: [{'pageNumber': 1, 'polygon': [4.2384, 9.9619, 7.762, 9.9619, 7.762, 10.3472, 4.2384, 10.3472]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 1, 'polygon': [4.2035, 6.9812, 7.6635, 6.9782, 7.6632, 9.8421, 4.2035, 9.8439]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (4.2035, 6.9812, 7.6632, 9.8421)\n",
      "\tFigure 0 cropped and saved as data/cropped\\sam_cropped_image_0.png\n",
      "\tDescription of figure 0: The bar chart titled \"Papers Released over Years\" visualizes the trend of research papers released from 2018 to 2023, categorized by three keywords: \"Large Language Model\" (LLMs), \"Large Language Model + Fine-Tuning\", and \"Large Language Model + Alignment\".\n",
      "\n",
      "- The x-axis represents the years from 2018 to 2023.\n",
      "- The y-axis represents the number of papers.\n",
      "\n",
      "The chart uses three different colors to distinguish between the categories:\n",
      "- Blue bars for \"LLMs\"\n",
      "- Red bars for \"LLMs + Fine-Tuning\"\n",
      "- Green bars for \"LLMs + Alignment\"\n",
      "\n",
      "Here's a year-by-year breakdown:\n",
      "\n",
      "- **2018**: \n",
      "  - LLMs: 42 papers\n",
      "  - LLMs + Fine-Tuning: 12 papers\n",
      "  - LLMs + Alignment: 3 papers\n",
      "\n",
      "- **2019**:\n",
      "  - LLMs: 60 papers\n",
      "  - LLMs + Fine-Tuning: 32 papers\n",
      "  - LLMs + Alignment: 17 papers\n",
      "\n",
      "- **2020**:\n",
      "  - LLMs: 114 papers\n",
      "  - LLMs + Fine-Tuning: 66 papers\n",
      "  - LLMs + Alignment: 26 papers\n",
      "\n",
      "- **2021**:\n",
      "  - LLMs: 260 papers\n",
      "  - LLMs + Fine-Tuning: 153 papers\n",
      "  - LLMs + Alignment: 58 papers\n",
      "\n",
      "- **2022**:\n",
      "  - LLMs: 1210 papers\n",
      "  - LLMs + Fine-Tuning: 582 papers\n",
      "  - LLMs + Alignment: 238 papers\n",
      "\n",
      "- **2023**:\n",
      "  - LLMs: 20900 papers\n",
      "  - LLMs + Fine-Tuning: 7260 papers\n",
      "  - LLMs + Alignment: 4740 papers\n",
      "\n",
      "The trend shows a significant increase in the number of papers over the years, particularly in 2023, which has a steep rise in the number of papers in all three categories compared to previous years. \"LLMs\" dominate the trend with the highest count, followed by \"LLMs + Fine-Tuning,\" and lastly \"LLMs + Alignment.\"\n",
      "Figure #1 has the following spans: [{'offset': 4406, 'length': 1341}]\n",
      "Span #0: {'offset': 4406, 'length': 1341}\n",
      "Original figure content in markdown: er the years, particularly in 2023, which has a steep rise in the number of papers in all three categories compared to previous years. \"LLMs\" dominate the trend with the highest count, followed by \"LLMs + Fine-Tuning,\" and lastly \"LLMs + Alignment.\"\" --></figure>\n",
      "\n",
      "\n",
      "<!-- Footnote=\"\\*Equal contribution Email addresses: humza\\_naveed@yahoo. com (Humza Naveed), aukhanee@gmail. com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi Qiu), muhammad. saqib@data61.csiro. au (Muhammad Saqib), saeed. anwar@kfupm. edu. sa (Saeed Anwar), muhammad. usman@kfupm. edu. sa (Muhammad Usman), naveed. akhtar1@unimelb. edu. au (Naveed Akhtar),\" -->\n",
      "\n",
      "<!-- Footnote=\"nick. barnes@anu. edu. au (Nick Barnes), ajmal.mian@uwa. edu. au (Ajmal Mian)\" -->\n",
      "\n",
      "<!-- PageFooter=\"Preprint submitted to Elsevier\" -->\n",
      "\n",
      "<!-- PageFooter=\"April 11, 2024\" -->\n",
      ":selected: :unselected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :unselected: :unselected:<figure>\n",
      "\n",
      "![](figures/1)\n",
      "\n",
      "<!-- FigureContent=\"Alpaca (Mar) 00 LLAMA (Feb) CodeGen (Mar) GPT-NeoX-20B [Apr) HuaTuo (Apr) Xuan Yuan 2.0 (May) TK-Instruct (May) UL2 (May) Vicuna MPT (Jun) :unselected: Koala (May) GLM (Oct) CodeT5+ ak mTO (D\n",
      "\tCaption: Figure 2: Chronological display of LLM releases: blue cards represent 'pre-trained' models, while orange cards correspond to 'instruction-tuned' models. Models\n",
      "on the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-\n",
      "tuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.\n",
      "\tCaption bounding region: [{'pageNumber': 2, 'polygon': [0.5019, 3.1686, 7.767, 3.1686, 7.767, 3.5538, 0.5019, 3.5538]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 2, 'polygon': [0.4871, 0.5728, 7.4342, 0.5726, 7.4319, 2.9744, 0.4855, 2.9742]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.4871, 0.5728, 7.4319, 2.9744)\n",
      "\tFigure 1 cropped and saved as data/cropped\\sam_cropped_image_1.png\n",
      "\tDescription of figure 1: The image is a chronological display of large language model (LLM) releases from 2019 to 2024. It uses blue cards to denote 'pre-trained' models and orange cards to denote 'instruction-tuned' models. The models positioned in the top half of the timeline are open-source, while those in the bottom half are closed-source.\n",
      "\n",
      "Key observations from the trend are as follows:\n",
      "- **2019:** \n",
      "  - T5 (October) - Pre-trained, open-source.\n",
      "  \n",
      "- **2020:** \n",
      "  - mT5 (October) - Pre-trained, open-source.\n",
      "  - GPT-3 (May) - Pre-trained, closed-source.\n",
      "  \n",
      "- **2021:**\n",
      "  - T0 (October) - Instruction-tuned, open-source.\n",
      "  - PanGu-α (April), CPM-2 (June) - Pre-trained, closed-source.\n",
      "  - WebGPT (December) - Instruction-tuned, closed-source.\n",
      "  \n",
      "- **2022:** \n",
      "  - Significant increase in model releases, particularly pre-trained ones:\n",
      "    - Codex, ERNIE 3.0, Jurassic-1, HyperCLOVA, Yuan 1.0, Gopher, ERNIE 3.0 Titan, GLaM, LaMDA.\n",
      "  - Pre-trained, open-source: CodeGen (March), GPT-NeoX-20B (April), UL2 (May), GLM (October), OPT (November), Galactica (November).\n",
      "  - Instruction-tuned, open-source: TK-Instruct (May), mT0 (December), OPT-IML (December).\n",
      "  - Instruction-tuned, closed-source: Sparrow, FLAN-U-PaLM, ChatGPT (all from November).\n",
      "  \n",
      "- **2023:** \n",
      "  - Simultaneous rise in both instruction-tuned and open-source models:\n",
      "    - Instruction-tuned, open-source: Alpaca (March), HuaTuo (April), Vicuna, Koala, Wizard-LM, Wizard-Coder (June), Goat.\n",
      "    - Pre-trained, open-source: LLaMA (February), LLaMA 2 (July), Code Llama (August).\n",
      "    - Instruction-tuned, closed-source: Bard (October).\n",
      "    - Pre-trained, closed-source: MT-NLG (January), AlphaCode (February), Chinchilla (March), PaLM (April), AlexaTM (August), U-PaLM (October), BLOOM (November).\n",
      "    \n",
      "- **2024:**\n",
      "  - Anticipated releases feature both instruction-tuned and pre-trained models:\n",
      "    - Instruction-tuned, open-source: PanGu-Σ (March).\n",
      "    - Pre-trained, closed-source: BloombergGPT, GPT-4, Claude, PaLM2 (May), Gemini (December).\n",
      "\n",
      "This chart effectively visualizes the increasing trend towards more instruction-tuned and open-source models in the evolving field of natural language processing.\n",
      "Figure #2 has the following spans: [{'offset': 10716, 'length': 1299}]\n",
      "Span #0: {'offset': 10716, 'length': 1299}\n",
      "Original figure content in markdown: tiza- tion [44, 45], knowledge distillation, and context length inter- polation [46, 47, 48, 49] among others are some of the methods widely studied for efficient LLM utilization.\n",
      "\n",
      "Due to the success of LLMs on a wide variety of tasks, the research literature has recently experienced a large influx of LLM-related contributions. Researchers have organized the LLMs literature in surveys [50, 51, 52, 53], and topic-specific surveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our contribution focuses on providing a comprehensive yet concise overview of the general direction of LLM research. This arti- cle summarizes architectural and training details of pre-trained LLMs and delves deeper into the details of concepts like fine- tuning, multi-modal LLMs, augmented LLMs, datasets, eval- uation, applications, challenges, and others to provide a self- contained comprehensive overview. Our key contributions are summarized as follows.\n",
      "\n",
      ". We present a survey on the developments in LLM research providing a concise comprehensive overview of the direc- tion.\n",
      "\n",
      "· We present extensive summaries of pre-trained models that include fine-grained details of architecture and training de- tails.\n",
      "\n",
      "· We summarize major findings of the popular contributions and provide a detailed discussion on t\n",
      "\tCaption: Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\n",
      "7. Challenges\n",
      "\tCaption bounding region: [{'pageNumber': 3, 'polygon': [0.5121, 8.2433, 7.7569, 8.2433, 7.7569, 8.4917, 0.5121, 8.4917]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 3, 'polygon': [1.1674, 1.2116, 7.2447, 1.2221, 7.2423, 8.0641, 1.1666, 8.0498]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (1.1674, 1.2116, 7.2423, 8.0641)\n",
      "\tFigure 2 cropped and saved as data/cropped\\sam_cropped_image_2.png\n",
      "\tDescription of figure 2: The image is a diagram offering an overview of Large Language Models (LLMs). This overview categorizes LLMs into seven main branches and further details subcategories within each branch:\n",
      "\n",
      "1. **Pre-Training**:\n",
      "   - **Single-Modal**: Examples mentioned include T5, mT5, GPT-3, PaLM, LLaMA, LLaMA-2, PaLM-2, U-PaLM, UL2.\n",
      "   - **Multi-Modal**: Examples provided are GPT-4, Gemini, PaLM-E, LaVIN, MiniGPT-4.\n",
      "\n",
      "2. **Fine-Tuning**:\n",
      "   - **Transfer Learning**\n",
      "   - **Instruction**: Can be either manual or automated.\n",
      "   - **Alignment**: Examples include RLHF, DPO, RLAIF, RAFT.\n",
      "   - **PEFT**: Includes LoRa, Adapter, Prefix, Prompt.\n",
      "\n",
      "3. **Efficient**:\n",
      "   - **Quantization**: Types are Post-Training and Quant-Aware.\n",
      "   - **Pruning**: Types are Structured and Unstructured.\n",
      "\n",
      "4. **Inference**:\n",
      "   - **Prompting**: Includes Multi-Turn Instructions.\n",
      "   - **Text Completion**: Includes Single Turn Instructions.\n",
      "\n",
      "5. **Evaluation**:\n",
      "   - **NLG**: Tasks include Summarization, Translation, Sentence Completion.\n",
      "   - **NLU**: Tasks include Classification, Sentiment Analysis, Common Sense Reasoning.\n",
      "   - **Alignment**: Values assessed include honesty, harmlessness, usefulness, safety, toxicity, adversarial attacks, and alignment tax.\n",
      "   - **Dataset**: Concerns include biasness, memorization, personal information, and toxicity.\n",
      "\n",
      "6. **Challenges**:\n",
      "   - **Training**\n",
      "   - **Inference**\n",
      "   - **Limited Memory**\n",
      "   - **Hardware**\n",
      "   - **Electricity**\n",
      "\n",
      "7. **Applications**:\n",
      "   - General Purpose, Medical, Education, Science, Maths, Law, Finance, Robotics, and Coding.\n",
      "   - Also, various methods such as QA, CoT, Reasoning, Zero-Shot, ICL, ToT, Multi-Modal, Task Planning, Tool Augmentation, Alignment, and Retrieval Augmentation.\n",
      "\n",
      "This diagram gives a comprehensive breakdown of the various aspects and considerations related to LLMs, from training and fine-tuning to their efficient use, evaluation, challenges, and applications.\n",
      "Figure #3 has the following spans: [{'offset': 21070, 'length': 526}]\n",
      "Span #0: {'offset': 21070, 'length': 526}\n",
      "Original figure content in markdown: library provides access to various pre- trained transformer models with APIs to train, fine-tune, infer, and develop custom models.\n",
      "\n",
      "DeepSpeed [36]: A library for scalable distributed training and inference of deep learning models.\n",
      "\n",
      "Megatron-LM [80]: It provides GPU-optimized techniques for large-scale training of LLMs.\n",
      "\n",
      "JAX [83]: A Python library for high-performance numerical computing and scaleable machine learning. It can differenti- ate native Python and NumPy functions and execute them on GPUs.\n",
      "\n",
      "Colossal-AI [84]: A\n",
      "\tCaption: Figure 4: An example of attention patterns in language models, image is taken\n",
      "from [93].\n",
      "\tCaption bounding region: [{'pageNumber': 5, 'polygon': [4.2486, 2.5146, 7.7569, 2.5146, 7.7569, 2.763, 4.2486, 2.763]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 5, 'polygon': [4.3607, 1.1424, 7.7046, 1.1421, 7.7052, 2.3605, 4.3612, 2.3606]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (4.3607, 1.1424, 7.7052, 2.3605)\n",
      "\tFigure 3 cropped and saved as data/cropped\\sam_cropped_image_3.png\n",
      "\tDescription of figure 3: The image displays three different attention pattern diagrams commonly used in language models: \n",
      "\n",
      "1. **Causal Decoder**: The diagram on the left shows the attention pattern for a causal decoder. The pattern is triangular, indicating that each token in the sequence can only attend to token positions before it and including it. This setup generally prevents a token from attending to future tokens, which is typical in autoregressive models.\n",
      "\n",
      "2. **Non-causal Decoder**: The middle diagram illustrates the attention pattern for a non-causal decoder. Here, each token can attend to all other tokens in the sequence, represented by a fully filled grid. This means that each token can attend bidirectionally to both previous and future tokens within the context.\n",
      "\n",
      "3. **Encoder-Decoder**: The diagram on the right represents the attention pattern for an encoder-decoder model. The encoder attends to the entire input sequence bi-directionally (fully filled grid), while the decoder's attention is again triangular, indicating that each token in the decoder can only attend to previous tokens up to its position, similar to the causal decoder pattern. The decoder can also attend to the entire output from the encoder as shown.\n",
      "\n",
      "All diagrams involve tokens labeled \"I\", \"am\", \"a\", and \"causal decoder\" (or \"NC decoder\" for the non-causal decoder and \"encoder decoder\" for the encoder-decoder), showing examples of how these models handle attention over a sequence of tokens. The attention mechanism is typically visualized with yellow boxes indicating positions of attention.\n",
      "Figure #4 has the following spans: [{'offset': 23749, 'length': 651}]\n",
      "Span #0: {'offset': 23749, 'length': 651}\n",
      "Original figure content in markdown: gureContent=\"The image displays three different attention pattern diagrams commonly used in language models: \n",
      "\n",
      "1. **Causal Decoder**: The diagram on the left shows the attention pattern for a causal decoder. The pattern is triangular, indicating that each token in the sequence can only attend to token positions before it and including it. This setup generally prevents a token from attending to future tokens, which is typical in autoregressive models.\n",
      "\n",
      "2. **Non-causal Decoder**: The middle diagram illustrates the attention pattern for a non-causal decoder. Here, each token can attend to all other tokens in the sequence, represented by a fully f\n",
      "\tCaption: Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\n",
      "different training stages like pre-training, instruction-tuning, or alignment tuning. \"RL\" stands for reinforcement learning, \"RM\" represents reward-modeling, and\n",
      "\"RLHF\" represents reinforcement learning with human feedback.\n",
      "\tCaption bounding region: [{'pageNumber': 6, 'polygon': [0.4918, 6.2509, 7.762, 6.2509, 7.762, 6.6413, 0.4918, 6.6413]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 6, 'polygon': [0.6105, 1.2655, 7.4714, 1.2701, 7.4708, 5.9175, 0.6124, 5.9107]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.6105, 1.2655, 7.4708, 5.9175)\n",
      "\tFigure 4 cropped and saved as data/cropped\\sam_cropped_image_4.png\n",
      "\tDescription of figure 4: The image is a flow diagram illustrating the different stages in the development and utilization of Large Language Models (LLMs). The stages are outlined as follows from left to right:\n",
      "\n",
      "1. **Pre-Training**: The initial stage where data is used for pre-training the model.\n",
      "  \n",
      "2. **Instruction-Tuning**: Following pre-training, the model undergoes instruction-tuning.\n",
      "\n",
      "3. **Reward Modeling**: This stage includes the construction of a reward model. It is depicted in a dashed blue box at the bottom which includes:\n",
      "    - **LLM**\n",
      "    - **Human Labeller**\n",
      "    - **Reward Model**\n",
      "\n",
      "4. **Reinforcement Learning (RL) Update**: This marks an interaction where the output of reward modeling feeds back into the LLM for further adjustments, forming a part of reinforcement learning with human feedback (RLHF).\n",
      "\n",
      "5. **Reinforcement Learning with Human Feedback (RLHF)**: Enclosed in a red dashed box, RLHF includes both the reward modeling and an additional green dashed box that shows:\n",
      "    - **LLM**\n",
      "    - **Reward** (interacting back and forth with the LLM)\n",
      "\n",
      "6. **Aligned LLM**: The output from the RLHF stage yields an aligned version of the LLM.\n",
      "\n",
      "7. **Prompting**: The final stage where the aligned LLM can be used for generating responses based on prompts, resulting in LLM Output.\n",
      "\n",
      "The diagram signifies that prompting can occur at various stages, including post-pre-training, instruction-tuning, or RLHF stage.\n",
      "Figure #5 has the following spans: [{'offset': 32609, 'length': 587}]\n",
      "Span #0: {'offset': 32609, 'length': 587}\n",
      "Original figure content in markdown: ponses according to human preferences using a classification objec- tive. To train the classifier humans annotate LLMs generated responses based on the HHH criteria.\n",
      "\n",
      "Reinforcement learning: in combination with the reward model is used for alignment in the next stage. The previously trained reward model ranks LLM-generated responses into preferred vs. non-preferred, which is used to align the model with proxi- mal policy optimization (PPO). This process repeats iteratively until convergence.\n",
      "\n",
      "\n",
      "#### 2.12.3. Prompting/Utilization\n",
      "\n",
      "Prompting is a method to query trained LLMs for gene\n",
      "\tCaption: Figure 7: Unified text-to-text training example, source image from [10].\n",
      "\tCaption bounding region: [{'pageNumber': 8, 'polygon': [0.6591, 2.5095, 3.8683, 2.5095, 3.8683, 2.6362, 0.6591, 2.6362]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 8, 'polygon': [0.5956, 1.229, 3.9216, 1.1845, 3.9338, 2.2977, 0.6088, 2.3412]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.5956, 1.229, 3.9338, 2.2977)\n",
      "\tFigure 5 cropped and saved as data/cropped\\sam_cropped_image_5.png\n",
      "\tDescription of figure 5: The image is a diagram illustrating different applications of the T5 (Text-to-Text Transfer Transformer) model for various natural language processing tasks. The central part of the diagram is labeled \"T5,\" and arrows connect this label to several examples of input and output pairs for training the model.\n",
      "\n",
      "1. At the top of the diagram:\n",
      "   - Input: \"translate English to German: That is good.\"\n",
      "   - Output: \"Das ist gut.\"\n",
      "\n",
      "2. To the left:\n",
      "   - Input: \"cola sentence: The course is jumping well.\"\n",
      "   - Output: \"not acceptable\" (actual language acceptability judgment task)\n",
      "\n",
      "3. Below that:\n",
      "   - Input: \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\"\n",
      "   - Output: \"3.8\" (semantic textual similarity task)\n",
      "\n",
      "4. At the bottom:\n",
      "   - Input: \"summarize: state authorities dispatched emergency crews Tuesday to survey the damage after an onslaught of severe weather in Mississippi…\"\n",
      "   - Output: \"six people hospitalized after a storm in Attala county.\"\n",
      "\n",
      "The diagram demonstrates the T5 model's versatility by showing how it can be trained to perform a wide range of text-based tasks using a unified text-to-text approach.\n",
      "Figure #6 has the following spans: [{'offset': 33196, 'length': 286}]\n",
      "Span #0: {'offset': 33196, 'length': 286}\n",
      "Original figure content in markdown: rating responses, as illustrated in Figure 6. LLMs can be prompted in various prompt setups, where they can be adapted to the instruc- tions without fine-tuning and in other cases with fine-tuning on data containing different prompt styles [16, 101, 102]. A good guide on prompt enginee\n",
      "\tCaption: Figure 8: The image is the article of [108], showing an example of PanGu-o\n",
      "architecture.\n",
      "\tCaption bounding region: [{'pageNumber': 8, 'polygon': [0.507, 5.1102, 4.0153, 5.1102, 4.0153, 5.3587, 0.507, 5.3587]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 8, 'polygon': [0.5959, 2.8132, 3.9899, 2.8114, 3.9903, 4.9229, 0.5965, 4.9241]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (0.5959, 2.8132, 3.9903, 4.9229)\n",
      "\tFigure 6 cropped and saved as data/cropped\\sam_cropped_image_6.png\n",
      "\tDescription of figure 6: The image is a diagram illustrating the architecture of PanGu-o. It represents how input tokens are processed through multiple layers.\n",
      "\n",
      "- **Position and Token Layer**: At the bottom, there are tokens represented by ovals with corresponding position numbers. They include:\n",
      "  - BOS (Beginning Of Sentence) at position 0\n",
      "  - \"the\" at position 1\n",
      "  - \"cat\" at position 2\n",
      "  - \"sat\" at position 3\n",
      "  - \"on\" at position 4\n",
      "  - \"the\" at position 5\n",
      "\n",
      "- **Transformer Layers**: Above the token and position layer, there are multiple interconnected green rectangular boxes arranged in rows and columns. These represent the transformer layers, where each box in a column corresponds to a specific processing unit or attention head within that layer.\n",
      "\n",
      "- **Query Layer**: At the top, there is a single rectangular blue box labeled \"Query layer\". This box has connections that lead to the last transformer layer.\n",
      "\n",
      "- **Final Output**: The final output \"mat\" (inside an oval) is generated at position 6, connected to the query layer output.\n",
      "\n",
      "The lines between the layers indicate how each token or unit at one stage connects to multiple units at the subsequent stage, demonstrating the network's attention mechanism that allows it to capture relationships between tokens. The architecture showcases the flow of information from positional encoding and token input through transformer layers to produce an output.\n",
      "Figure #7 has the following spans: [{'offset': 41513, 'length': 437}]\n",
      "Span #0: {'offset': 41513, 'length': 437}\n",
      "Original figure content in markdown: re updated by inserting prompts at various positions, front, middle, and back. CPM-2 also pro- poses the INFMOE, a memory-efficient framework with a strat- egy to dynamically offload parameters to the CPU for inference at a 100B scale. It overlaps data movement with inference com- putation for lower inference time.\n",
      "\n",
      "ERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi- task learning to build a modular architecture using Transforme\n",
      "\tCaption: Figure 9: The BLOOM architecture example sourced from [13].\n",
      "\tCaption bounding region: [{'pageNumber': 9, 'polygon': [4.573, 2.9556, 7.4274, 2.9556, 7.4274, 3.0773, 4.573, 3.0773]}]\n",
      "\tFigure body bounding regions: {'pageNumber': 9, 'polygon': [4.3345, 1.1629, 7.7198, 1.1618, 7.72, 2.8206, 4.3349, 2.821]}\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (4.3345, 1.1629, 7.72, 2.8206)\n",
      "\tFigure 7 cropped and saved as data/cropped\\sam_cropped_image_7.png\n",
      "\tDescription of figure 7: This image depicts the architecture of BLOOM, a transformer-based model. The architecture is broken down into several key components:\n",
      "\n",
      "1. **Decoder Block (x70):** On the left side, there are multiple (70) decoder blocks stacked vertically. Each block processes tokens (Token_1, Token_2, Token_3, etc.) using a series of operations:\n",
      "   - **Embedding (Embedᴛ):** The tokens are first passed through an embedding layer.\n",
      "   - **Layer Normalization (LN):** Post embedding, each token representation undergoes layer normalization.\n",
      "   - **Softmax:** Finally, a softmax operation is applied.\n",
      "\n",
      "2. **Multi-Head Attention:** The outputs from the decoder blocks are passed to a multi-head attention mechanism. Details of each head include:\n",
      "   - Each head contains MLP (Multi-Layer Perceptron) layers and Layer Normalization (LN) layers, suggesting a sequence of operations applied to the representations in the attention heads.\n",
      "\n",
      "3. **ALIBI Mask:** Attention components depict a masked attention mechanism using an ALIBI mask:\n",
      "   - The ALIBI mask matrix shows values affecting how the attention scores are computed, with example values like 0, -∞ corresponding to different heads (kᵤₑₐₑₑ).\n",
      "\n",
      "4. **Other Components:**\n",
      "   - **Key-Query Product:** This operation computes the attention scores.\n",
      "   - **Softmax:** This is used after computing the key-query product.\n",
      "   - **Weighted Sum of Values:** Combines values in a weighted manner based on the computed attention scores.\n",
      "   - **Head Fusion:** Finally, there is a head fusion step that combines the results from different attention heads.\n",
      "\n",
      "Each of these elements contributes to the overall processing pipeline within the BLOOM transformer architecture.\n"
     ]
    }
   ],
   "source": [
    "loader = AzureAIDocumentIntelligenceLoader(file_path=r'C:\\Manoranjan\\Code\\Personal\\entity_extraction\\sam.pdf', \n",
    "                                           api_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"), \n",
    "                                           api_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\"),\n",
    "                                           api_model=\"prebuilt-layout\",\n",
    "                                           api_version=\"2024-02-29-preview\",\n",
    "                                           mode='markdown',\n",
    "                                           analysis_features = [DocumentAnalysisFeature.OCR_HIGH_RESOLUTION])\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87844736-64d5-496e-8205-3408e20a7fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:09:43.480895Z",
     "iopub.status.busy": "2024-07-19T13:09:43.480295Z",
     "iopub.status.idle": "2024-07-19T13:09:43.490331Z",
     "shell.execute_reply": "2024-07-19T13:09:43.489178Z",
     "shell.execute_reply.started": "2024-07-19T13:09:43.480895Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A Comprehensive Overview of Large Language Models\n",
       "===\n",
       "\n",
       "Humza Naveedª, Asad Ullah Khana,\\*, Shi Qiub,\\*, Muhammad Saqibc,d,\\*, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i, Nick Barnesh, Ajmal Miani\n",
       "\n",
       "a University of Engineering and Technology (UET), Lahore, Pakistan bThe Chinese University of Hong Kong (CUHK), HKSAR, China c University of Technology Sydney (UTS), Sydney, Australia\n",
       "\n",
       "d Commonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia e King Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia fSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
       "\n",
       "& The University of Melbourne (UoM), Melbourne, Australia\n",
       "\n",
       "hAustralian National University (ANU), Canberra, Australia\n",
       "\n",
       "The University of Western Australia (UWA), Perth, Australia\n",
       "\n",
       "arXiv:2307.06435v9 [cs.CL] 9 Apr 2024\n",
       "\n",
       "\n",
       "# Abstract\n",
       "\n",
       "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.\n",
       "\n",
       "\n",
       "# Keywords: Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\n",
       "\n",
       "\n",
       "# 1\\. Introduction\n",
       "\n",
       "Language plays a fundamental role in facilitating commu- nication and self-expression for humans, and their interaction with machines. The need for generalized models stems from the growing demand for machines to handle complex language tasks, including translation, summarization, information re- trieval, conversational interactions, etc. Recently, significant breakthroughs have been witnessed in language models, pri- marily attributed to transformers [1], increased computational capabilities, and the availability of large-scale training data. These developments have brought about a revolutionary trans- formation by enabling the creation of LLMs that can approxi- mate human-level performance on various tasks [2, 3]. Large\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/0)<!-- FigureContent=\"The bar chart titled \"Papers Released over Years\" visualizes the trend of research papers released from 2018 to 2023, categorized by three keywords: \"Large Language Model\" (LLMs), \"Large Language Model + Fine-Tuning\", and \"Large Language Model + Alignment\".\n",
       "\n",
       "- The x-axis represents the years from 2018 to 2023.\n",
       "- The y-axis represents the number of papers.\n",
       "\n",
       "The chart uses three different colors to distinguish between the categories:\n",
       "- Blue bars for \"LLMs\"\n",
       "- Red bars for \"LLMs + Fine-Tuning\"\n",
       "- Green bars for \"LLMs + Alignment\"\n",
       "\n",
       "Here's a year-by-year breakdown:\n",
       "\n",
       "- **2018**: \n",
       "  - LLMs: 42 papers\n",
       "  - LLMs + Fine-Tuning: 12 papers\n",
       "  - LLMs + Alignment: 3 papers\n",
       "\n",
       "- **2019**:\n",
       "  - LLMs: 60 papers\n",
       "  - LLMs + Fine-Tuning: 32 papers\n",
       "  - LLMs + Alignment: 17 papers\n",
       "\n",
       "- **2020**:\n",
       "  - LLMs: 114 papers\n",
       "  - LLMs + Fine-Tuning: 66 papers\n",
       "  - LLMs + Alignment: 26 papers\n",
       "\n",
       "- **2021**:\n",
       "  - LLMs: 260 papers\n",
       "  - LLMs + Fine-Tuning: 153 papers\n",
       "  - LLMs + Alignment: 58 papers\n",
       "\n",
       "- **2022**:\n",
       "  - LLMs: 1210 papers\n",
       "  - LLMs + Fine-Tuning: 582 papers\n",
       "  - LLMs + Alignment: 238 papers\n",
       "\n",
       "- **2023**:\n",
       "  - LLMs: 20900 papers\n",
       "  - LLMs + Fine-Tuning: 7260 papers\n",
       "  - LLMs + Alignment: 4740 papers\n",
       "\n",
       "The trend shows a significant increase in the number of papers over the years, particularly in 2023, which has a steep rise in the number of papers in all three categories compared to previous years. \"LLMs\" dominate the trend with the highest count, followed by \"LLMs + Fine-Tuning,\" and lastly \"LLMs + Alignment.\"\" --></figure>\n",
       "\n",
       "\n",
       "<!-- Footnote=\"\\*Equal contribution Email addresses: humza\\_naveed@yahoo. com (Humza Naveed), aukhanee@gmail. com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi Qiu), muhammad. saqib@data61.csiro. au (Muhammad Saqib), saeed. anwar@kfupm. edu. sa (Saeed Anwar), muhammad. usman@kfupm. edu. sa (Muhammad Usman), naveed. akhtar1@unimelb. edu. au (Naveed Akhtar),\" -->\n",
       "\n",
       "<!-- Footnote=\"nick. barnes@anu. edu. au (Nick Barnes), ajmal.mian@uwa. edu. au (Ajmal Mian)\" -->\n",
       "\n",
       "<!-- PageFooter=\"Preprint submitted to Elsevier\" -->\n",
       "\n",
       "<!-- PageFooter=\"April 11, 2024\" -->\n",
       ":selected: :unselected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :unselected: :unselected:<figure>\n",
       "\n",
       "![](figures/1)<!-- FigureContent=\"The image is a chronological display of large language model (LLM) releases from 2019 to 2024. It uses blue cards to denote 'pre-trained' models and orange cards to denote 'instruction-tuned' models. The models positioned in the top half of the timeline are open-source, while those in the bottom half are closed-source.\n",
       "\n",
       "Key observations from the trend are as follows:\n",
       "- **2019:** \n",
       "  - T5 (October) - Pre-trained, open-source.\n",
       "  \n",
       "- **2020:** \n",
       "  - mT5 (October) - Pre-trained, open-source.\n",
       "  - GPT-3 (May) - Pre-trained, closed-source.\n",
       "  \n",
       "- **2021:**\n",
       "  - T0 (October) - Instruction-tuned, open-source.\n",
       "  - PanGu-α (April), CPM-2 (June) - Pre-trained, closed-source.\n",
       "  - WebGPT (December) - Instruction-tuned, closed-source.\n",
       "  \n",
       "- **2022:** \n",
       "  - Significant increase in model releases, particularly pre-trained ones:\n",
       "    - Codex, ERNIE 3.0, Jurassic-1, HyperCLOVA, Yuan 1.0, Gopher, ERNIE 3.0 Titan, GLaM, LaMDA.\n",
       "  - Pre-trained, open-source: CodeGen (March), GPT-NeoX-20B (April), UL2 (May), GLM (October), OPT (November), Galactica (November).\n",
       "  - Instruction-tuned, open-source: TK-Instruct (May), mT0 (December), OPT-IML (December).\n",
       "  - Instruction-tuned, closed-source: Sparrow, FLAN-U-PaLM, ChatGPT (all from November).\n",
       "  \n",
       "- **2023:** \n",
       "  - Simultaneous rise in both instruction-tuned and open-source models:\n",
       "    - Instruction-tuned, open-source: Alpaca (March), HuaTuo (April), Vicuna, Koala, Wizard-LM, Wizard-Coder (June), Goat.\n",
       "    - Pre-trained, open-source: LLaMA (February), LLaMA 2 (July), Code Llama (August).\n",
       "    - Instruction-tuned, closed-source: Bard (October).\n",
       "    - Pre-trained, closed-source: MT-NLG (January), AlphaCode (February), Chinchilla (March), PaLM (April), AlexaTM (August), U-PaLM (October), BLOOM (November).\n",
       "    \n",
       "- **2024:**\n",
       "  - Anticipated releases feature both instruction-tuned and pre-trained models:\n",
       "    - Instruction-tuned, open-source: PanGu-Σ (March).\n",
       "    - Pre-trained, closed-source: BloombergGPT, GPT-4, Claude, PaLM2 (May), Gemini (December).\n",
       "\n",
       "This chart effectively visualizes the increasing trend towards more instruction-tuned and open-source models in the evolving field of natural language processing.\" --></figure>\n",
       "\n",
       "\n",
       "Language Models (LLMs) have emerged as cutting-edge arti- ficial intelligence systems that can process and generate text with coherent communication [4], and generalize to multiple tasks [5, 6].\n",
       "\n",
       "The historical progress in natural language processing (NLP) evolved from statistical to neural language modeling and then from pre-trained language models (PLMs) to LLMs. While conventional language modeling (LM) trains task-specific mod- els in supervised settings, PLMs are trained in a self-supervised setting on a large corpus of text [7, 8, 9] with the aim of learning a generic representation that is shareable among various NLP tasks. After fine-tuning for downstream tasks, PLMs surpass the performance gains of traditional language modeling (LM). The larger PLMs bring more performance gains, which has led to the transitioning of PLMs to LLMs by significantly increas- ing model parameters (tens to hundreds of billions) [10] and training dataset (many GBs and TBs) [10, 11]. Following this development, numerous LLMs have been proposed in the lit- erature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the number of released LLMs and names of a few significant LLMs proposed over the years are shown in Fig 1 and Fig 2, respec- tively.\n",
       "\n",
       "The early work on LLMs, such as T5 [10] and mT5 [11] em- ployed transfer learning until GPT-3 [6] showed LLMs are zero-shot transferable to downstream tasks without fine-tuning. LLMs accurately respond to task queries when prompted with task descriptions and examples. However, pre-trained LLMs fail to follow user intent and perform worse in zero-shot set- tings than in few-shot. Fine-tuning them with task instruc- tions data [16, 17, 18, 19] and aligning with human prefer- ences [20, 21] enhances generalization to unseen tasks, im- proving zero-shot performance significantly and reducing mis- aligned behavior.\n",
       "\n",
       "In addition to better generalization and domain adaptation, LLMs appear to have emergent abilities, such as reasoning, planning, decision-making, in-context learning, answering in zero-shot settings, etc. These abilities are known to be ac- quired by them due to their gigantic scale even when the pre- trained LLMs are not trained specifically to possess these at- tributes [22, 23, 24]. Such abilities have led LLMs to be widely\n",
       "\n",
       "adopted in diverse settings including, multi-modal, robotics, tool manipulation, question answering, autonomous agents, etc. Various improvements have also been suggested in these areas either by task-specific training [25, 26, 27, 28, 29, 30, 31] or better prompting [32].\n",
       "\n",
       "The LLMs abilities to solve diverse tasks with human-level performance come at a cost of slow training and inference, extensive hardware requirements, and higher running costs. Such requirements have limited their adoption and opened up opportunities to devise better architectures [15, 33, 34, 35] and training strategies [36, 37, 21, 38, 39, 40, 41]. Param- eter efficient tuning [38, 41, 40], pruning [42, 43], quantiza- tion [44, 45], knowledge distillation, and context length inter- polation [46, 47, 48, 49] among others are some of the methods widely studied for efficient LLM utilization.\n",
       "\n",
       "Due to the success of LLMs on a wide variety of tasks, the research literature has recently experienced a large influx of LLM-related contributions. Researchers have organized the LLMs literature in surveys [50, 51, 52, 53], and topic-specific surveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our contribution focuses on providing a comprehensive yet concise overview of the general direction of LLM research. This arti- cle summarizes architectural and training details of pre-trained LLMs and delves deeper into the details of concepts like fine- tuning, multi-modal LLMs, augmented LLMs, datasets, eval- uation, applications, challenges, and others to provide a self- contained comprehensive overview. Our key contributions are summarized as follows.\n",
       "\n",
       ". We present a survey on the developments in LLM research providing a concise comprehensive overview of the direc- tion.\n",
       "\n",
       "· We present extensive summaries of pre-trained models that include fine-grained details of architecture and training de- tails.\n",
       "\n",
       "· We summarize major findings of the popular contributions and provide a detailed discussion on the key design and development aspects of LLMs to help practitioners effec- tively leverage this technology.\n",
       "\n",
       "· In this self-contained article, we cover a range of con- cepts to present the general direction of LLMs compre-\n",
       "\n",
       "<!-- PageNumber=\"2\" -->\n",
       ":selected: :selected: :unselected: :selected: :unselected: :selected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected: :unselected: :selected: :unselected: :selected: :selected: :unselected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected:<figure>\n",
       "\n",
       "![](figures/2)<!-- FigureContent=\"The image is a diagram offering an overview of Large Language Models (LLMs). This overview categorizes LLMs into seven main branches and further details subcategories within each branch:\n",
       "\n",
       "1. **Pre-Training**:\n",
       "   - **Single-Modal**: Examples mentioned include T5, mT5, GPT-3, PaLM, LLaMA, LLaMA-2, PaLM-2, U-PaLM, UL2.\n",
       "   - **Multi-Modal**: Examples provided are GPT-4, Gemini, PaLM-E, LaVIN, MiniGPT-4.\n",
       "\n",
       "2. **Fine-Tuning**:\n",
       "   - **Transfer Learning**\n",
       "   - **Instruction**: Can be either manual or automated.\n",
       "   - **Alignment**: Examples include RLHF, DPO, RLAIF, RAFT.\n",
       "   - **PEFT**: Includes LoRa, Adapter, Prefix, Prompt.\n",
       "\n",
       "3. **Efficient**:\n",
       "   - **Quantization**: Types are Post-Training and Quant-Aware.\n",
       "   - **Pruning**: Types are Structured and Unstructured.\n",
       "\n",
       "4. **Inference**:\n",
       "   - **Prompting**: Includes Multi-Turn Instructions.\n",
       "   - **Text Completion**: Includes Single Turn Instructions.\n",
       "\n",
       "5. **Evaluation**:\n",
       "   - **NLG**: Tasks include Summarization, Translation, Sentence Completion.\n",
       "   - **NLU**: Tasks include Classification, Sentiment Analysis, Common Sense Reasoning.\n",
       "   - **Alignment**: Values assessed include honesty, harmlessness, usefulness, safety, toxicity, adversarial attacks, and alignment tax.\n",
       "   - **Dataset**: Concerns include biasness, memorization, personal information, and toxicity.\n",
       "\n",
       "6. **Challenges**:\n",
       "   - **Training**\n",
       "   - **Inference**\n",
       "   - **Limited Memory**\n",
       "   - **Hardware**\n",
       "   - **Electricity**\n",
       "\n",
       "7. **Applications**:\n",
       "   - General Purpose, Medical, Education, Science, Maths, Law, Finance, Robotics, and Coding.\n",
       "   - Also, various methods such as QA, CoT, Reasoning, Zero-Shot, ICL, ToT, Multi-Modal, Task Planning, Tool Augmentation, Alignment, and Retrieval Augmentation.\n",
       "\n",
       "This diagram gives a comprehensive breakdown of the various aspects and considerations related to LLMs, from training and fine-tuning to their efficient use, evaluation, challenges, and applications.\" --></figure>\n",
       "\n",
       "\n",
       "hensively, including background, pre-training, fine-tuning, multi-modal LLMs, augmented LLMs, LLMs-powered agents, datasets, evaluation, etc.\n",
       "\n",
       "We loosely follow the existing terminology to ensure a stan- dardized outlook of this research direction. For instance, fol- lowing [50], our survey discusses pre-trained LLMs with 10B parameters or more. We refer the readers interested in smaller pre-trained models to [51, 52, 53].\n",
       "\n",
       "The organization of this paper is as follows. Section 2 discusses the background of LLMs. Section 3 focuses on LLMs overview,\n",
       "\n",
       "architectures, training pipelines and strategies, fine-tuning, and utilization in different domains. Section 4 highlights the config- uration and parameters that play a crucial role in the function- ing of these models. Summary and discussions are presented in section 3.8. The LLM training and evaluation, datasets, and benchmarks are discussed in section 5, followed by challenges and future directions, and conclusion in sections 7 and 8, re- spectively.\n",
       "\n",
       "<!-- PageNumber=\"3\" -->\n",
       "\n",
       "# 2\\. Background\n",
       "\n",
       "We provide the relevant background to understand the fun- damentals related to LLMs in this section. We briefly discuss necessary components in LLMs and refer the readers interested in details to the original works.\n",
       "\n",
       "\n",
       "## 2.1. Tokenization\n",
       "\n",
       "Tokenization [59] is an essential pre-processing step in LLM training that parses the text into non-decomposing units called tokens. Tokens can be characters, subwords [60], sym- bols [61], or words, depending on the tokenization process. Some of the commonly used tokenization schemes in LLMs include wordpiece [62], byte pair encoding (BPE) [61], and un- igramLM [60]. Readers are encouraged to refer to [63] for a detailed survey.\n",
       "\n",
       "\n",
       "## 2.2. Encoding Positions\n",
       "\n",
       "The transformer processes input sequences in parallel and independently of each other. Moreover, the attention mod- ule in the transformer does not capture positional information. As a result, positional encodings were introduced in trans- former [64], where a positional embedding vector is added to the token embedding. Variants of positional embedding include absolute, relative, or learned positional encodings. Within rel- ative encoding, Alibi and RoPE are two widely used positional embeddings in LLMs.\n",
       "\n",
       "Alibi [65]: It subtracts a scalar bias from the attention score that increases with the distance between token positions. This favors using recent tokens for attention.\n",
       "\n",
       "ROPE [66]: It rotates query and key representations at an an- gle proportional to the token absolute position in the input sequence, resulting in a relative positional encoding scheme which decays with the distance between the tokens.\n",
       "\n",
       "\n",
       "## 2.3. Attention in LLMs\n",
       "\n",
       "Attention assigns weights to input tokens based on impor- tance so that the model gives more emphasis to relevant tokens. Attention in transformers [64] calculates query, key, and value mappings for input sequences, where the attention score is obtained by multiplying the query and key, and later used to weight values. We discuss different attention strategies used in LLMs below.\n",
       "\n",
       "Self-Attention [64]: Calculates attention using queries, keys, and values from the same block (encoder or decoder).\n",
       "\n",
       "Cross Attention: It is used in encoder-decoder architectures, where encoder outputs are the queries, and key-value pairs come from the decoder.\n",
       "\n",
       "Sparse Attention [67]: Self-attention has O(n2) time complex- ity which becomes infeasible for large sequences. To speed up the computation, sparse attention [67] iteratively calculates attention in sliding windows for speed gains.\n",
       "\n",
       "Flash Attention [68]: Memory access is the major bottleneck in calculating attention using GPUs. To speed up, flash attention employs input tiling to minimize the memory reads and writes between the GPU high bandwidth memory (HBM) and the on-chip SRAM.\n",
       "\n",
       "\n",
       "## 2.4. Activation Functions\n",
       "\n",
       "The activation functions serve a crucial role in the curve- fitting abilities of neural networks [69]. We discuss activation functions used in LLMs in this section.\n",
       "\n",
       "ReLU [70]: The Rectified linear unit (ReLU) is defined as:\n",
       "\n",
       "ReLU(x) = max(0, x) (1)\n",
       "\n",
       "GeLU [7]]: The Gaussian Error Linear Unit (GeLU) is the combination of ReLU, dropout [72] and zoneout [73].\n",
       "\n",
       "GLU variants [74]: The Gated Linear Unit [75] is a neural network layer that is an element-wise product (0) of a linear transformation and a sigmoid transformed (o) linear projection of the input given as:\n",
       "\n",
       "GLU(x,W,V,b,c) =(xW+b)@@(xV+c), (2)\n",
       "\n",
       "where X is the input of layer and I, W, b, V and c are learned parameters. Other GLU variants [74] used in LLMs are:\n",
       "\n",
       "ReGLU(x, W, V,b,c) =max(0, xW +b)@,\n",
       "\n",
       "GEGLU(x,W,V,b,c)=GELU(xW+b)®(xV+c), SwiGLU(x, W,V,b,c,B) =Swishß(xW+b)®(xV+c).\n",
       "\n",
       "\n",
       "## 2.5. Layer Normalization\n",
       "\n",
       "Layer normalization leads to faster convergence and is an in- tegrated component of transformers [64]. In addition to Layer- Norm [76] and RMSNorm [77], LLMs use pre-layer normal- ization [78], applying it before multi-head attention (MHA). Pre-norm is shown to provide training stability in LLMs. An- other normalization variant, DeepNorm [79] fixes the issue with larger gradients in pre-norm.\n",
       "\n",
       "\n",
       "## 2.6. Distributed LLM Training\n",
       "\n",
       "This section describes distributed LLM training approaches briefly. More details are available in [13, 37, 80, 81].\n",
       "\n",
       "Data Parallelism: Data parallelism replicates the model on multiple devices where data in a batch gets divided across de- vices. At the end of each training iteration weights are synchro- nized across all devices.\n",
       "\n",
       "Tensor Parallelism: Tensor parallelism shards a tensor compu- tation across devices. It is also known as horizontal parallelism or intra-layer model parallelism.\n",
       "\n",
       "Pipeline Parallelism: Pipeline parallelism shards model layers across different devices. This is also known as vertical paral- lelism.\n",
       "\n",
       "Model Parallelism: A combination of tensor and pipeline par- allelism is known as model parallelism.\n",
       "\n",
       "3D Parallelism: A combination of data, tensor, and model par- allelism is known as 3D parallelism.\n",
       "\n",
       "Optimizer Parallelism: Optimizer parallelism also known as zero redundancy optimizer [37] implements optimizer state partitioning, gradient partitioning, and parameter partitioning across devices to reduce memory consumption while keeping the communication costs as low as possible.\n",
       "\n",
       "<!-- PageNumber=\"4\" -->\n",
       "\n",
       "## 2.7. Libraries\n",
       "\n",
       "Some commonly used libraries for LLMs training are: Transformers [82]: The library provides access to various pre- trained transformer models with APIs to train, fine-tune, infer, and develop custom models.\n",
       "\n",
       "DeepSpeed [36]: A library for scalable distributed training and inference of deep learning models.\n",
       "\n",
       "Megatron-LM [80]: It provides GPU-optimized techniques for large-scale training of LLMs.\n",
       "\n",
       "JAX [83]: A Python library for high-performance numerical computing and scaleable machine learning. It can differenti- ate native Python and NumPy functions and execute them on GPUs.\n",
       "\n",
       "Colossal-AI [84]: A collection of components to write dis- tributed deep learning models.\n",
       "\n",
       "BMTrain [81]: A library to write efficient stand-alone LLMs training code.\n",
       "\n",
       "FastMoE [85]: Provides API to build mixture-of-experts (MoE) model in PyTorch.\n",
       "\n",
       "MindSpore [86]: A deep learning training and inference frame- work extendable to mobile, edge, and cloud computing.\n",
       "\n",
       "PyTorch [87]: A framework developed by Facebook AI Re- search lab (FAIR) to build deep learning models. The main features of PyTorch include a dynamic computation graph and a pythonic coding style.\n",
       "\n",
       "Tensorflow [88]: A deep learning framework written by Google. The key features of TensorFlow are graph-based com- putation, eager execution, scalability, etc.\n",
       "\n",
       "MXNet [89]: Apache MXNet is a deep learning framework with support to write programs in multiple languages, includ- ing, Python, C++, Scala, R, etc. It also provides support for dynamic and static computation graphs.\n",
       "\n",
       "\n",
       "## 2.8. Data PreProcessing\n",
       "\n",
       "This section briefly summarizes data preprocessing tech- niques used in LLMs training.\n",
       "\n",
       "Quality Filtering: For better results, training data quality is essential. Some approaches to filtering data are: 1) classifier- based and 2) heuristics-based. Classifier-based approaches train a classifier on high-quality data and predict the quality of text for filtering, whereas heuristics-based employ some rules for filtering like language, metrics, statistics, and keywords.\n",
       "\n",
       "Data Deduplication: Duplicated data can affect model per- formance and increase data memorization; therefore, to train LLMs, data deduplication is one of the preprocessing steps. This can be performed at multiple levels, like sentences, documents, and datasets.\n",
       "\n",
       "Privacy Reduction: Most of the training data for LLMs is collected through web sources. This data contains private information; therefore, many LLMs employ heuristics-based methods to filter information such as names, addresses, and phone numbers to avoid learning personal information.\n",
       "\n",
       "\n",
       "## 2.9. Architectures\n",
       "\n",
       "Here we discuss the variants of the transformer architectures used in LLMs. The difference arises due to the application of\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/3)<!-- FigureContent=\"The image displays three different attention pattern diagrams commonly used in language models: \n",
       "\n",
       "1. **Causal Decoder**: The diagram on the left shows the attention pattern for a causal decoder. The pattern is triangular, indicating that each token in the sequence can only attend to token positions before it and including it. This setup generally prevents a token from attending to future tokens, which is typical in autoregressive models.\n",
       "\n",
       "2. **Non-causal Decoder**: The middle diagram illustrates the attention pattern for a non-causal decoder. Here, each token can attend to all other tokens in the sequence, represented by a fully filled grid. This means that each token can attend bidirectionally to both previous and future tokens within the context.\n",
       "\n",
       "3. **Encoder-Decoder**: The diagram on the right represents the attention pattern for an encoder-decoder model. The encoder attends to the entire input sequence bi-directionally (fully filled grid), while the decoder's attention is again triangular, indicating that each token in the decoder can only attend to previous tokens up to its position, similar to the causal decoder pattern. The decoder can also attend to the entire output from the encoder as shown.\n",
       "\n",
       "All diagrams involve tokens labeled \"I\", \"am\", \"a\", and \"causal decoder\" (or \"NC decoder\" for the non-causal decoder and \"encoder decoder\" for the encoder-decoder), showing examples of how these models handle attention over a sequence of tokens. The attention mechanism is typically visualized with yellow boxes indicating positions of attention.\" --></figure>\n",
       "\n",
       "\n",
       "targets\n",
       "\n",
       "Full Language Modeling\n",
       "\n",
       "May the force be with you\n",
       "\n",
       "targets\n",
       "\n",
       "Prefix Language Modeling\n",
       "\n",
       "May the force be with you\n",
       "\n",
       "targets\n",
       "\n",
       "Masked Language Modeling\n",
       "\n",
       "May the force be with you\n",
       "\n",
       "Figure 5: An example of language model training objectives, image from [93].\n",
       "\n",
       "the attention and the connection of transformer blocks. An il- lustration of attention patterns of these architectures is shown in Figure 4.\n",
       "\n",
       "Encoder Decoder: This architecture processes inputs through the encoder and passes the intermediate representation to the decoder to generate the output. Here, the encoder sees the complete sequence utilizing self-attention whereas the decoder processes the sequence one after the other with implementing cross-attention.\n",
       "\n",
       "Causal Decoder: A type of architecture that does not have an encoder and processes and generates output using a decoder, where the predicted token depends only on the previous time steps.\n",
       "\n",
       "Prefix Decoder: It is also known as a non-causal decoder, where the attention calculation is not strictly dependent on the past information and the attention is bidirectional. An example of a non-causal attention mask is shown in Figure 4.\n",
       "\n",
       "Mixture-of-Experts: It is a variant of transformer architecture with parallel independent experts and a router to route tokens to experts. These experts are feed-forward layers after the at- tention block [90]. Mixture-of-Experts (MoE) is an efficient sparse architecture that offers comparable performance to dense models and allows increasing the model size without increas- ing the computational cost by activating only a few experts at a time [91, 92].\n",
       "\n",
       "\n",
       "## 2.10. Pre-Training Objectives\n",
       "\n",
       "This section describes LLMs pre-training objectives. For more details see the paper [93].\n",
       "\n",
       "Full Language Modeling: An autoregressive language model- ing objective where the model is asked to predict future tokens given the previous tokens, an example is shown in Figure 5.\n",
       "\n",
       "Prefix Language Modeling: A non-causal training objective, where a prefix is chosen randomly and only remaining target tokens are used to calculate the loss. An example is shown in Figure 5.\n",
       "\n",
       "<!-- PageNumber=\"5\" -->\n",
       ":unselected:<figure>\n",
       "\n",
       "![](figures/4)<!-- FigureContent=\"The image is a flow diagram illustrating the different stages in the development and utilization of Large Language Models (LLMs). The stages are outlined as follows from left to right:\n",
       "\n",
       "1. **Pre-Training**: The initial stage where data is used for pre-training the model.\n",
       "  \n",
       "2. **Instruction-Tuning**: Following pre-training, the model undergoes instruction-tuning.\n",
       "\n",
       "3. **Reward Modeling**: This stage includes the construction of a reward model. It is depicted in a dashed blue box at the bottom which includes:\n",
       "    - **LLM**\n",
       "    - **Human Labeller**\n",
       "    - **Reward Model**\n",
       "\n",
       "4. **Reinforcement Learning (RL) Update**: This marks an interaction where the output of reward modeling feeds back into the LLM for further adjustments, forming a part of reinforcement learning with human feedback (RLHF).\n",
       "\n",
       "5. **Reinforcement Learning with Human Feedback (RLHF)**: Enclosed in a red dashed box, RLHF includes both the reward modeling and an additional green dashed box that shows:\n",
       "    - **LLM**\n",
       "    - **Reward** (interacting back and forth with the LLM)\n",
       "\n",
       "6. **Aligned LLM**: The output from the RLHF stage yields an aligned version of the LLM.\n",
       "\n",
       "7. **Prompting**: The final stage where the aligned LLM can be used for generating responses based on prompts, resulting in LLM Output.\n",
       "\n",
       "The diagram signifies that prompting can occur at various stages, including post-pre-training, instruction-tuning, or RLHF stage.\" --></figure>\n",
       "\n",
       "\n",
       "Masked Language Modeling: In this training objective, tokens or spans (a sequence of tokens) are masked randomly and the model is asked to predict masked tokens given the past and future context. An example is shown in Figure 5.\n",
       "\n",
       "Unified Language Modeling: Unified language modeling [94] is a combination of causal, non-causal, and masked language training objectives. Here in masked language modeling, the attention is not bidirectional but unidirectional, attending either left-to-right or right-to-left context.\n",
       "\n",
       "\n",
       "### 2.11. LLMs Scaling Laws\n",
       "\n",
       "Scaling laws study the optimal combination of model param- eters, dataset size, and computational resources that predict the improvement in the model performance. It has been shown that the loss scales according to the power-law with model size, dataset size, and compute resources [95]. This study suggests larger models are more important than big data for better perfor- mance. Another variant of scaling law [96] suggests the model size and the number of training tokens should be scaled equally.\n",
       "\n",
       "\n",
       "### 2.12. LLMs Adaptation Stages\n",
       "\n",
       "This section discusses the fundamentals of LLMs adaptation stages, from pre-training to fine-tuning for downstream tasks and utilization. An example of different training stages and in- ference in LLMs is shown in Figure 6. In this paper, we refer to alignment-tuning as aligning with human preferences, while occasionally the literature uses the term alignment for different purposes.\n",
       "\n",
       "\n",
       "#### 2.12.1. Pre-Training\n",
       "\n",
       "In the very first stage, the model is trained in a self- supervised manner on a large corpus to predict the next to- kens given the input. The design choices of LLMs vary from encoder-decoder to decoder-only architectures with different building blocks and loss functions in sections 2.5, 2.4, 2.10.\n",
       "\n",
       "\n",
       "#### 2.12.2. Fine-Tuning\n",
       "\n",
       "There are different styles to fine-tune an LLM. This section briefly discusses fine-tuning approaches.\n",
       "\n",
       "Transfer Learning: The pre-trained LLMs perform well for various tasks [6, 15]. However, to improve the performance for\n",
       "\n",
       "<!-- PageNumber=\"6\" -->\n",
       ":unselected: :unselected: :selected: :unselected: :unselected: :unselected:\n",
       "a downstream task, pre-trained models are fine-tuned with the task-specific data [10, 11], known as transfer learning.\n",
       "\n",
       "Instruction-tuning: To enable a model to respond to user queries effectively, the pre-trained model is fine-tuned on in- struction formatted data i.e., instruction and an input-output pair. Instructions generally comprise multi-task data in plain natural language, guiding the model to respond according to the prompt and the input. This type of fine-tuning improves zero- shot generalization and downstream task performance. Details on formatting instruction data and its various styles are avail- able in [16, 50, 97].\n",
       "\n",
       "Alignment-tuning: LLMs are prone to generating false, biased, and harmful text. To make them helpful, honest, and harmless, models are aligned using human feedback. Alignment involves asking LLMs to generate unexpected responses and then updat- ing their parameters to avoid such responses [20, 21, 98].\n",
       "\n",
       "It ensures LLMs operate according to human intentions and values. A model is defined to be an \"aligned\" model if the model fulfills three criteria of helpful, honest, and harmless or \"HHH\" [99].\n",
       "\n",
       "Researchers employ reinforcement learning with human feed- back (RLHF) [100] for model alignment. In RLHF, a fine-tuned model on demonstrations is further trained with reward model- ing (RM) and reinforcement learning (RL), shown in Figure 6. Below we briefly discuss RM and RL pipelines in RLHF. Reward modeling: trains a model to rank generated responses according to human preferences using a classification objec- tive. To train the classifier humans annotate LLMs generated responses based on the HHH criteria.\n",
       "\n",
       "Reinforcement learning: in combination with the reward model is used for alignment in the next stage. The previously trained reward model ranks LLM-generated responses into preferred vs. non-preferred, which is used to align the model with proxi- mal policy optimization (PPO). This process repeats iteratively until convergence.\n",
       "\n",
       "\n",
       "#### 2.12.3. Prompting/Utilization\n",
       "\n",
       "Prompting is a method to query trained LLMs for generating responses, as illustrated in Figure 6. LLMs can be prompted in various prompt setups, where they can be adapted to the instruc- tions without fine-tuning and in other cases with fine-tuning on data containing different prompt styles [16, 101, 102]. A good guide on prompt engineering is available at [32]. Below, we will discuss various widely used prompt setups.\n",
       "\n",
       "Zero-Shot Prompting: LLMs are zero-shot learners and ca- pable of answering queries never seen before. This style of prompting requires LLMs to answer user questions without see- ing any examples in the prompt.\n",
       "\n",
       "In-context Learning: Also known as few-shot learning, here, multiple input-output demonstration pairs are shown to the model to generate the desired response. This adaptation style is also called few-shot learning. A discussion on formatting in- context learning (ICL) templates is available in [54, 50, 18, 16]. Reasoning in LLMs: LLMs are zero-shot reasoners and can be provoked to generate answers to logical problems, task planning, critical thinking, etc. with reasoning. Generating reasons is possible only by using different prompting styles,\n",
       "\n",
       "whereas to improve LLMs further on reasoning tasks many methods [16, 97] train them on reasoning datasets. We discuss various prompting techniques for reasoning below.\n",
       "\n",
       "Chain-of-Thought (CoT): A special case of prompting where demonstrations contain reasoning information aggregated with inputs and outputs so that the model generates outcomes with step-by-step reasoning. More details on CoT prompts are avail- able in [55, 103, 101].\n",
       "\n",
       "Self-Consistency: Improves CoT performance by generat- ing multiple responses and selecting the most frequent an- swer [104].\n",
       "\n",
       "Tree-of-Thought (ToT): Explores multiple reasoning paths with possibilities to look ahead and backtrack for problem- solving [105].\n",
       "\n",
       "Single-Turn Instructions: In this prompting setup, LLMs are queried only once with all the relevant information in the prompt. LLMs generate responses by understanding the con- text either in a zero-shot or few-shot setting.\n",
       "\n",
       "Multi-Turn Instructions: Solving a complex task requires mul- tiple interactions with LLMs, where feedback and responses from the other tools are given as input to the LLM for the next rounds. This style of using LLMs in the loop is common in autonomous agents.\n",
       "\n",
       "\n",
       "# 3\\. Large Language Models\n",
       "\n",
       "This section reviews LLMs, briefly describing their architec- tures, training objectives, pipelines, datasets, and fine-tuning details.\n",
       "\n",
       "\n",
       "## 3.1. Pre-Trained LLMs\n",
       "\n",
       "Here, we provide summaries of various well-known pre- trained LLMs with significant discoveries, changing the course of research and development in NLP. These LLMs have consid- erably improved the performance in NLU and NLG domains, and are widely fine-tuned for downstream tasks. Moreover, We also identify key findings and insights of pre-trained LLMs in Table 1 and 2 that improve their performance.\n",
       "\n",
       "\n",
       "### 3.1.1. General Purpose\n",
       "\n",
       "T5 [10]: An encoder-decoder model employing a unified text- to-text training for all NLP problems is shown in Figure 7. T5 places layer normalization outside the residual path in a conven- tional transformer model [64]. It uses masked language mod- eling as a pre-training objective where spans (consecutive to- kens) are replaced with a single mask instead of separate masks for each token. This type of masking speeds up the training as it produces shorter sequences. After pre-training, the model is fine-tuned using adapter layers [106] for downstream tasks.\n",
       "\n",
       "GPT-3 [6]: The GPT-3 architecture is the same as the GPT- 2 [5] but with dense and sparse attention in transformer layers similar to the Sparse Transformer [67]. It shows that large mod- els can train on larger batch sizes with a lower learning rate to decide the batch size during training, GPT-3 uses the gradient noise scale as in [107]. Overall, GPT-3 increases model param- eters to 175B showing that the performance of large language\n",
       "\n",
       "<!-- PageNumber=\"7\" -->\n",
       "<figure>\n",
       "\n",
       "![](figures/5)<!-- FigureContent=\"The image is a diagram illustrating different applications of the T5 (Text-to-Text Transfer Transformer) model for various natural language processing tasks. The central part of the diagram is labeled \"T5,\" and arrows connect this label to several examples of input and output pairs for training the model.\n",
       "\n",
       "1. At the top of the diagram:\n",
       "   - Input: \"translate English to German: That is good.\"\n",
       "   - Output: \"Das ist gut.\"\n",
       "\n",
       "2. To the left:\n",
       "   - Input: \"cola sentence: The course is jumping well.\"\n",
       "   - Output: \"not acceptable\" (actual language acceptability judgment task)\n",
       "\n",
       "3. Below that:\n",
       "   - Input: \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\"\n",
       "   - Output: \"3.8\" (semantic textual similarity task)\n",
       "\n",
       "4. At the bottom:\n",
       "   - Input: \"summarize: state authorities dispatched emergency crews Tuesday to survey the damage after an onslaught of severe weather in Mississippi…\"\n",
       "   - Output: \"six people hospitalized after a storm in Attala county.\"\n",
       "\n",
       "The diagram demonstrates the T5 model's versatility by showing how it can be trained to perform a wide range of text-based tasks using a unified text-to-text approach.\" --></figure>\n",
       "\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/6)<!-- FigureContent=\"The image is a diagram illustrating the architecture of PanGu-o. It represents how input tokens are processed through multiple layers.\n",
       "\n",
       "- **Position and Token Layer**: At the bottom, there are tokens represented by ovals with corresponding position numbers. They include:\n",
       "  - BOS (Beginning Of Sentence) at position 0\n",
       "  - \"the\" at position 1\n",
       "  - \"cat\" at position 2\n",
       "  - \"sat\" at position 3\n",
       "  - \"on\" at position 4\n",
       "  - \"the\" at position 5\n",
       "\n",
       "- **Transformer Layers**: Above the token and position layer, there are multiple interconnected green rectangular boxes arranged in rows and columns. These represent the transformer layers, where each box in a column corresponds to a specific processing unit or attention head within that layer.\n",
       "\n",
       "- **Query Layer**: At the top, there is a single rectangular blue box labeled \"Query layer\". This box has connections that lead to the last transformer layer.\n",
       "\n",
       "- **Final Output**: The final output \"mat\" (inside an oval) is generated at position 6, connected to the query layer output.\n",
       "\n",
       "The lines between the layers indicate how each token or unit at one stage connects to multiple units at the subsequent stage, demonstrating the network's attention mechanism that allows it to capture relationships between tokens. The architecture showcases the flow of information from positional encoding and token input through transformer layers to produce an output.\" --></figure>\n",
       "\n",
       "\n",
       "models improves with the scale and is competitive with the fine- tuned models.\n",
       "\n",
       "mT5 [11]: A multilingual T5 model [10] trained on the mC4 dataset with 101 languages. The dataset is extracted from the public common crawl scrape. The model uses a larger vocab- ulary size of 250,000 to cover multiple languages. To avoid over-fitting or under-fitting for a language, mT5 employs a data sampling procedure to select samples from all languages. The paper suggests using a small amount of pre-training datasets, including all languages when fine-tuning for a task using En- glish language data. This allows the model to generate correct non-English outputs.\n",
       "\n",
       "PanGu-& [108]: An autoregressive model that has a query layer at the end of standard transformer layers, example shown in Figure 8, to predict the next token. Its structure is similar to the transformer layer but with an additional embedding for the next position in the attention mechanism, given in Eq. 3.\n",
       "\n",
       "a = PnWĄWĄTHE\n",
       "\n",
       "(3)\n",
       "\n",
       "CPM-2 [12]: Cost-efficient Pre-trained language Models (CPM-2) pre-trains bilingual (English and Chinese) 11B and 198B mixture-of-experts (MoE) models on the WuDaoCor- pus [109] dataset. The tokenization process removes \"\\_\" white space tokens in the sentencepiece tokenizer. The models are trained with knowledge inheritance, starting with only the Chi- nese language in the first stage and then adding English and Chinese data. This trained model gets duplicated multiple times to initialize the 198B MoE model. Moreover, to use the model for downstream tasks, CPM-2 experimented with both com-\n",
       "\n",
       "plete fine-tuning and prompt fine-tuning as in [40] where only prompt-related parameters are updated by inserting prompts at various positions, front, middle, and back. CPM-2 also pro- poses the INFMOE, a memory-efficient framework with a strat- egy to dynamically offload parameters to the CPU for inference at a 100B scale. It overlaps data movement with inference com- putation for lower inference time.\n",
       "\n",
       "ERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi- task learning to build a modular architecture using Transformer- XL [111] as the backbone. The universal representation mod- ule is shared by all the tasks, which serve as the basic block for task-specific representation modules, which are all trained jointly for natural language understanding, natural language generation, and knowledge extraction. This LLM is primar- ily focused on the Chinese language. It claims to train on the largest Chinese text corpora for LLM training, and achieved state-of-the-art in 54 Chinese NLP tasks.\n",
       "\n",
       "Jurassic-1 [112]: A pair of auto-regressive language mod- els, including a 7B-parameter J1-Large model and a 178B- parameter J1-Jumbo model. The training vocabulary of Jurassic-1 comprise word pieces, complete words, and multi- word expressions without any word boundaries, where possible out-of-vocabulary instances are interpreted as Unicode bytes. Compared to the GPT-3 counterparts, the Jurassic-1 models apply a more balanced depth-to-width self-attention architec- ture [113] and an improved tokenizer for a faster prediction based on broader resources, achieving a comparable perfor- mance in zero-shot learning tasks and a superior performance in few-shot learning tasks given the ability to feed more examples as a prompt.\n",
       "\n",
       "HyperCLOVA [114]: A Korean language model with GPT-3 architecture.\n",
       "\n",
       "Yuan 1.0 [115]: Trained on a Chinese corpus with 5TB of high-quality text collected from the Internet. A Massive Data Filtering System (MDFS) built on Spark is developed to pro- cess the raw data via coarse and fine filtering techniques. To speed up the training of Yuan 1.0 to save energy expenses and carbon emissions, various factors that improve the performance of distributed training are incorporated in architecture and train- ing: like increasing the hidden state size improves pipeline and tensor parallelism performance, larger micro batches improve pipeline parallelism performance, and larger global batch size improve data parallelism performance. In practice, the Yuan 1.0 model performs well on text classification, Winograd Schema, natural language inference, and reading comprehension tasks.\n",
       "\n",
       "Gopher [116]: The Gopher family of models ranges from 44M to 280B parameters in size to study the effect of scale on the LLMs performance. The 280B model beats GPT-3 [6], Jurrasic-1 [112], MT-NLG [117], and others on 81% of the evaluated tasks.\n",
       "\n",
       "ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0 by training a larger model with 26x the number of parameters of the latter. This bigger model outperformed other state-of-the- art models in 68 NLP tasks. LLMs produce text with incorrect facts. In order to have control of the generated text with fac- tual consistency, ERNIE 3.0 Titan adds another task, Credible and Controllable Generations, to its multi-task learning setup.\n",
       "\n",
       "<!-- PageNumber=\"8\" -->\n",
       ":selected: :selected: :selected: :selected: :selected: :selected: :selected:\n",
       "It introduces additional self-supervised adversarial and control- lable language modeling losses to the pre-training step, which enables ERNIE 3.0 Titan to beat other LLMs in their manually selected Factual QA task set evaluations.\n",
       "\n",
       "GPT-NeoX-20B [118]: An auto-regressive model that largely follows GPT-3 with a few deviations in architecture design, trained on the Pile dataset without any data deduplication. GPT- NeoX has parallel attention and feed-forward layers in a trans- former block, given in Eq. 4, that increases throughput by 15%. It uses rotary positional embedding [66], applying it to only 25% of embedding vector dimension as in [119]. This reduces the computation without performance degradation. As opposed to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B uses only dense layers. The hyperparameter tuning at this scale is difficult; therefore, the model chooses hyperparameters from the method [6] and interpolates values between 13B and 175B models for the 20B model. The model training is distributed among GPUs using both tensor and pipeline parallelism.\n",
       "\n",
       "\n",
       "# x+ Attn(LN1(x))+ FF(LN2(x))\n",
       "\n",
       "(4)\n",
       "\n",
       "OPT [14]: It is a clone of GPT-3, developed to open-source a model that replicates GPT-3 performance. Training of OPT employs dynamic loss scaling [120] and restarts from an earlier checkpoint with a lower learning rate whenever loss divergence is observed. Overall, the performance of OPT-175B models is comparable to the GPT3-175B model.\n",
       "\n",
       "BLOOM [13]: A causal decoder model trained on the ROOTS corpus to open-source an LLM. The architecture of BLOOM is shown in Figure 9, with differences like ALiBi positional em- bedding, an additional normalization layer after the embedding layer as suggested by the bitsandbytes1 library. These changes stabilize training with improved downstream performance.\n",
       "\n",
       "GLaM [91]: Generalist Language Model (GLaM) represents a family of language models using a sparsely activated decoder- only mixture-of-experts (MoE) structure [121, 90]. To gain more model capacity while reducing computation, the experts are sparsely activated where only the best two experts are used to process each input token. The largest GLAM model, GLaM (64B/64E), is about 7x larger than GPT-3 [6], while only part of the parameters are activated per input token. The largest GLaM (64B/64E) model achieves better overall results as compared to GPT-3 while consuming only one-third of GPT-3's training energy.\n",
       "\n",
       "MT-NLG [117]: A 530B causal decoder based on the GPT- 2 architecture that has roughly 3x GPT-3 model parameters. MT-NLG is trained on filtered high-quality data collected from various public datasets and blends various types of datasets in a single batch, which beats GPT-3 on several evaluations.\n",
       "\n",
       "Chinchilla [96]: A causal decoder trained on the same dataset as the Gopher [116] but with a little different data sampling distribution (sampled from MassiveText). The model architec- ture is similar to the one used for Gopher, with the exception of AdamW optimizer instead of Adam. Chinchilla identifies the\n",
       "\n",
       "<figure>\n",
       "\n",
       "![](figures/7)<!-- FigureContent=\"This image depicts the architecture of BLOOM, a transformer-based model. The architecture is broken down into several key components:\n",
       "\n",
       "1. **Decoder Block (x70):** On the left side, there are multiple (70) decoder blocks stacked vertically. Each block processes tokens (Token_1, Token_2, Token_3, etc.) using a series of operations:\n",
       "   - **Embedding (Embedᴛ):** The tokens are first passed through an embedding layer.\n",
       "   - **Layer Normalization (LN):** Post embedding, each token representation undergoes layer normalization.\n",
       "   - **Softmax:** Finally, a softmax operation is applied.\n",
       "\n",
       "2. **Multi-Head Attention:** The outputs from the decoder blocks are passed to a multi-head attention mechanism. Details of each head include:\n",
       "   - Each head contains MLP (Multi-Layer Perceptron) layers and Layer Normalization (LN) layers, suggesting a sequence of operations applied to the representations in the attention heads.\n",
       "\n",
       "3. **ALIBI Mask:** Attention components depict a masked attention mechanism using an ALIBI mask:\n",
       "   - The ALIBI mask matrix shows values affecting how the attention scores are computed, with example values like 0, -∞ corresponding to different heads (kᵤₑₐₑₑ).\n",
       "\n",
       "4. **Other Components:**\n",
       "   - **Key-Query Product:** This operation computes the attention scores.\n",
       "   - **Softmax:** This is used after computing the key-query product.\n",
       "   - **Weighted Sum of Values:** Combines values in a weighted manner based on the computed attention scores.\n",
       "   - **Head Fusion:** Finally, there is a head fusion step that combines the results from different attention heads.\n",
       "\n",
       "Each of these elements contributes to the overall processing pipeline within the BLOOM transformer architecture.\" --></figure>\n",
       "\n",
       "\n",
       "relationship that model size should be doubled for every dou- bling of training tokens. Over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 bil- lion tokens are trained to get the estimates for compute-optimal training under a given budget. The authors train a 70B model with the same compute budget as Gopher (280B) but with 4 times more data. It outperforms Gopher [116], GPT-3 [6], and others on various downstream tasks, after fine-tuning.\n",
       "\n",
       "AlexaTM [122]: An encoder-decoder model, where encoder weights and decoder embeddings are initialized with a pre- trained encoder to speed up training. The encoder stays frozen for the initial 100k steps and is later unfrozen for end-to-end training. The model is trained on a combination of denoising and causal language modeling (CLM) objectives, concatenat- ing a [CLM] token at the beginning for mode switching. Dur- ing training, the CLM task is applied for 20% of the time, which improves the in-context learning performance.\n",
       "\n",
       "PaLM [15]: A causal decoder with parallel attention and feed-forward layers similar to Eq. 4, speeding up training by a factor of 15. Additional changes to the conventional trans- former model include SwiGLU activation, RoPE embeddings, multi-query attention that saves computation cost during decod- ing, and shared input-output embeddings. During training, loss spiking was observed, and to fix it, model training was restarted from a 100-step earlier checkpoint by skipping 200-500 batches around the spike. Moreover, the model was found to memo- rize around 2.4% of the training data at the 540B model scale, whereas this number was lower for smaller models.\n",
       "\n",
       "PaLM-2 [123]: A smaller multi-lingual variant of PaLM, trained for larger iterations on a better quality dataset. PaLM- 2 shows significant improvements over PaLM, while reducing training and inference costs due to its smaller size. To lessen toxicity and memorization, it appends special tokens with a fraction of pre-training data, which shows a reduction in gener- ating harmful responses.\n",
       "\n",
       "U-PaLM [124]: This method trains PaLM for 0.1% addi- tional compute with the UL2 (also named as UL2Restore) ob- jective [125], using the same dataset it outperforms the baseline significantly on various NLP tasks, including zero-shot, few- shot, commonsense reasoning, CoT, etc. Training with UL2R involves converting a causal decoder PaLM to a non-causal de- coder PaLM and employing 50% sequential denoising, 25% regular denoising, and 25% extreme denoising loss functions.\n",
       "\n",
       "<!-- Footnote=\"1 https://github.com/TimDettmers/bitsandbytes\" -->\n",
       "\n",
       "<!-- PageNumber=\"9\" -->\n",
       "\n",
       "UL2 [125]: An encoder-decoder architecture trained using a mixture of denoisers (MoD) objective. Denoisers include 1) R-Denoiser: a regular span masking, 2) S-Denoiser: which cor- rupts consecutive tokens of a large sequence and 3) X-Denoiser: which corrupts a large number of tokens randomly. During pre- training, UL2 includes a denoiser token from R, S, X to rep- resent a denoising setup. It helps improve fine-tuning perfor- mance for downstream tasks that bind the task to one of the up- stream training modes. This MoD style of training outperforms the T5 model on many benchmarks.\n",
       "\n",
       "GLM-130B [33]: GLM-130B is a bilingual (English and Chi- nese) model trained using an auto-regressive mask infilling pre- training objective similar to the GLM [126]. This training style makes the model bidirectional as compared to GPT-3, which is unidirectional. As opposed to GLM, the training of GLM-130B includes a small amount of multi-task instruction pre-training data (5% of the total data) along with self-supervised mask in- filling. To stabilize the training, it applies embedding layer gra- dient shrink.\n",
       "\n",
       "LLAMA [127, 21]: A set of decoder-only language models varying from 7B to 70B parameters. LLAMA models series is the most famous among the community for parameter efficiency and instruction tuning.\n",
       "\n",
       "LLAMA-1 [127]: Implements efficient causal attention [128] by not storing and computing masked attention weights and key/query scores. Another optimization is reducing the number of activations recomputed in the backward pass, as in [129].\n",
       "\n",
       "LLaMA-2 [21]: This work is more focused on fine-tuning a safer and better LLaMA-2-Chat model for dialogue generation. The pre-trained model has 40% more training data with a larger context length and grouped-query attention.\n",
       "\n",
       "PanGu-> [92]: An autoregressive model with parameters copied from PanGu-o and extended to a trillion scale with Ran- dom Routed Experts (RRE), the architectural diagram is shown in Figure 10. RRE is similar to the MoE architecture, with distinctions at the second level, where tokens are randomly routed to experts in a domain instead of using a learnable gat- ing method. The model has bottom layers densely activated and shared across all domains, whereas top layers are sparsely activated according to the domain. This training style allows extracting task-specific models and reduces catastrophic forget- ting effects in the case of continual learning.\n",
       "\n",
       "\n",
       "## 3.1.2. Coding\n",
       "\n",
       "CodeGen [130]: CodeGen has similar architecture to PaLM [15], i.e., parallel attention, MLP layers, and RoPE em- beddings. The model is trained on both natural language and programming language data sequentially (trained on the first dataset, then the second and so on) on the following datasets 1) PILE, 2) BIGQUERY and 3) BIGPYTHON. CodeGen pro- posed a multi-step approach to synthesizing code. The purpose is to simplify the generation of long sequences where the previ- ous prompt and generated code are given as input with the next prompt to generate the next code sequence. CodeGen open- source a Multi-Turn Programming Benchmark (MTPB) to eval- uate multi-step program synthesis.\n",
       "\n",
       "Codex [13]]: This LLM is trained on a subset of public Python Github repositories to generate code from docstrings. Com- puter programming is an iterative process where the programs are often debugged and updated before fulfilling the require- ments. Similarly to this, Codex generates 100 versions of a program by repetitive sampling for a given description, which produces a working solution for 77.5% of the problems passing unit tests. Its powerful version powers Github Copilot2.\n",
       "\n",
       "AlphaCode [132]: A set of large language models, ranging from 300M to 41B parameters, designed for competition-level code generation tasks. It uses the multi-query attention [133] to reduce memory and cache costs. Since competitive program- ming problems highly require deep reasoning and an under- standing of complex natural language algorithms, the Alpha- Code models are pre-trained on filtered GitHub code in popular languages and then fine-tuned on a new competitive program- ming dataset named CodeContests. The CodeContests dataset mainly contains problems, solutions, and test cases collected from the Codeforces platform3. The pre-training employs stan- dard language modeling objectives, while GOLD [134] with tempering [135] serves as the training objective for the fine- tuning on CodeContests data. To evaluate the performance of AlphaCode, simulated programming competitions are hosted on the Codeforces platform: overall, AlphaCode ranks at the top 54.3% among over 5000 competitors, where its Codeforces rating is within the top 28% of recently participated users.\n",
       "\n",
       "CodeT5+ [34]: CodeT5+ is based on CodeT5 [136], with shallow encoder and deep decoder, trained in multiple stages initially unimodal data (code) and later bimodal data (text-code pairs). Each training stage has different training objectives and activates different model blocks encoder, decoder, or both ac- cording to the task. The unimodal pre-training includes span denoising and CLM objectives, whereas bimodal pre-training objectives contain contrastive learning, matching, and CLM for text-code pairs. CodeT5+ adds special tokens with the text to enable task modes, for example, [CLS ] for contrastive loss, [Match] for text-code matching, etc.\n",
       "\n",
       "StarCoder [137]: A decoder-only model with the SantaCoder architecture, employing Flash attention to scale up the context length to 8k. The StarCoder trains an encoder to filter names, emails, and other personal data from the training data. Its fine- tuned variant outperforms PaLM, LLAMA, and LAMDA on HumanEval and MBPP benchmarks.\n",
       "\n",
       "\n",
       "### 3.1.3. Scientific Knowledge\n",
       "\n",
       "Galactica [138]: A large curated corpus of human scientific knowledge with 48 million papers, textbooks, lecture notes, millions of compounds and proteins, scientific websites, en- cyclopedias, and more are trained using the metaseq library3, which is built on PyTorch and fairscale [139]. The model wraps reasoning datasets with the < work > token to provide step-by- step reasoning context to the model, which has been shown to improve the performance on reasoning tasks.\n",
       "\n",
       "<!-- Footnote=\"2https://github.com/features/copilot\" -->\n",
       "\n",
       "<!-- Footnote=\"3https://codeforces.com/\" -->\n",
       "\n",
       "<!-- PageNumber=\"10\" -->\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(docs[-1].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b63875-247a-4313-be9a-f33f5b9ff1fc",
   "metadata": {},
   "source": [
    "### Split the document into chunks base on markdown headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b0347d-42c0-41e4-96ae-ce0806bb4b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:12:14.214871Z",
     "iopub.status.busy": "2024-07-19T13:12:14.213789Z",
     "iopub.status.idle": "2024-07-19T13:12:14.239284Z",
     "shell.execute_reply": "2024-07-19T13:12:14.237180Z",
     "shell.execute_reply.started": "2024-07-19T13:12:14.214871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits: 25\n"
     ]
    }
   ],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),  \n",
    "    (\"#######\", \"Header 7\"), \n",
    "    (\"########\", \"Header 8\")\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = docs[0].page_content\n",
    "docs_result = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(docs_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5704e50c-c7b5-44c7-865d-b9e4d658ebbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:12:26.474663Z",
     "iopub.status.busy": "2024-07-19T13:12:26.474140Z",
     "iopub.status.idle": "2024-07-19T13:12:26.486567Z",
     "shell.execute_reply": "2024-07-19T13:12:26.485012Z",
     "shell.execute_reply.started": "2024-07-19T13:12:26.474663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Header 1': '2\\\\. Background', 'Header 2': '2.7. Libraries'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_result[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc962d67-2cbb-4962-9d87-5a74ebd98e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:13:58.210953Z",
     "iopub.status.busy": "2024-07-19T13:13:58.209399Z",
     "iopub.status.idle": "2024-07-19T13:13:58.219798Z",
     "shell.execute_reply": "2024-07-19T13:13:58.216682Z",
     "shell.execute_reply.started": "2024-07-19T13:13:58.210953Z"
    }
   },
   "source": [
    "### Character Splitter to Split based on Chunk Size as well as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29494f5a-f48c-4ed2-8773-903507001cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:14:05.905609Z",
     "iopub.status.busy": "2024-07-19T13:14:05.904981Z",
     "iopub.status.idle": "2024-07-19T13:14:05.925837Z",
     "shell.execute_reply": "2024-07-19T13:14:05.923240Z",
     "shell.execute_reply.started": "2024-07-19T13:14:05.905609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits: 40\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "from langchain_text_splitters.base import Language, TextSplitter\n",
    "\n",
    "class CustomCharacterTextSplitter(TextSplitter):\n",
    "    \"\"\"Splitting text that looks at characters.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, separator: str = \"\\n\\n\", is_separator_regex: bool = False, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new TextSplitter.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self._separator = separator\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        # First we naively split the large input into a bunch of smaller ones.\n",
    "        separator = (\n",
    "            self._separator if self._is_separator_regex else re.escape(self._separator)\n",
    "        )\n",
    "        splits = re.split(separator, text, flags=re.DOTALL) \n",
    "        splits = [part for part in splits if part.strip()]\n",
    "        return splits\n",
    "\n",
    "text_splitter = CustomCharacterTextSplitter(separator=r'(<figure>.*?</figure>)', is_separator_regex=True)\n",
    "child_docs  = text_splitter.split_documents(docs_result)\n",
    "print(\"Length of splits: \" + str(len(child_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8c0da-fd73-4bd7-8ea2-fd47194d24ea",
   "metadata": {},
   "source": [
    "### Load the LangChain OpenAI Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad5fdb5-7131-446c-bb87-83cc9190f17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:14:42.964161Z",
     "iopub.status.busy": "2024-07-19T13:14:42.962610Z",
     "iopub.status.idle": "2024-07-19T13:14:45.075062Z",
     "shell.execute_reply": "2024-07-19T13:14:45.073372Z",
     "shell.execute_reply.started": "2024-07-19T13:14:42.963645Z"
    }
   },
   "outputs": [],
   "source": [
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2024-03-01-preview\",\n",
    "    azure_endpoint =os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098fec1-de27-4941-9d5f-226abf50abb2",
   "metadata": {},
   "source": [
    "### Create the Azure AI Search Index Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05359122-9bc3-43fc-9d78-515f6a2de8fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:15:48.289567Z",
     "iopub.status.busy": "2024-07-19T13:15:48.288476Z",
     "iopub.status.idle": "2024-07-19T13:15:50.254410Z",
     "shell.execute_reply": "2024-07-19T13:15:50.252071Z",
     "shell.execute_reply.started": "2024-07-19T13:15:48.289567Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ScoringProfile,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights\n",
    ")\n",
    "embedding_function = aoai_embeddings.embed_query\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embedding_function(\"Text\")),\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=False,\n",
    "    ),\n",
    "    # Additional field to store the title\n",
    "    SearchableField(\n",
    "        name=\"header\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field for filtering on document source\n",
    "    SimpleField(\n",
    "        name=\"image\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=False,\n",
    "        searchable=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76eebb-a56d-4b2d-a729-f1cd3c7eb195",
   "metadata": {},
   "source": [
    "### Create the AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb7b3eff-2ece-48cd-b370-0dc750bf738b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:16:46.780093Z",
     "iopub.status.busy": "2024-07-19T13:16:46.779569Z",
     "iopub.status.idle": "2024-07-19T13:16:50.715209Z",
     "shell.execute_reply": "2024-07-19T13:16:50.713577Z",
     "shell.execute_reply.started": "2024-07-19T13:16:46.780093Z"
    }
   },
   "outputs": [],
   "source": [
    "index_name: str = \"langchain-vector-demo-custom3\"\n",
    "\n",
    "vector_store_multi_modal: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.environ[\"AZURE_SEARCH_ENDPOINT\"],\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_KEY\"],\n",
    "    index_name=index_name,\n",
    "    embedding_function=embedding_function,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f8d686-f670-4378-8205-d6b8975c04d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:19:10.940104Z",
     "iopub.status.busy": "2024-07-19T13:19:10.939484Z",
     "iopub.status.idle": "2024-07-19T13:19:10.947995Z",
     "shell.execute_reply": "2024-07-19T13:19:10.945999Z",
     "shell.execute_reply.started": "2024-07-19T13:19:10.940104Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "def find_figure_indices(text):\n",
    "    pattern = r'!\\[\\]\\(figures/(\\d+)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    indices = [int(match) for match in matches]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10840850-da0e-4f35-b702-1b1c4dfd056a",
   "metadata": {},
   "source": [
    "### Ingest the chunks into Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c6974b2-304c-4ed4-9c81-6e7a3bf8829a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:19:32.525779Z",
     "iopub.status.busy": "2024-07-19T13:19:32.524183Z",
     "iopub.status.idle": "2024-07-19T13:19:49.334301Z",
     "shell.execute_reply": "2024-07-19T13:19:49.333189Z",
     "shell.execute_reply.started": "2024-07-19T13:19:32.525779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZTEyMTNiMzctMGNlOS00YmFhLWE4NGEtZDJlNmQ1NmM4ZjQ1',\n",
       " 'ZDQzMTU0NDktMmI2Ny00NDI2LTllOWQtODExYTA0YmQzMWIw',\n",
       " 'MmEyOWQyNDQtOWEwOC00MGNiLTg4NWYtMTNlZmVlMjBkYzFk',\n",
       " 'OTg0ODdjNjQtYmQ2Yy00MjhmLWFlMWUtYzU3NDEwNDI2ZjE0',\n",
       " 'OGM3MDU0NzYtODQ4MC00OTczLWJjYmItZTI3MjQ0ZjY5ZWJh',\n",
       " 'NGI3NmY0NGQtMTg3ZC00YWY5LTg1MjctYWNjMzNjYTIzOThh',\n",
       " 'YWEwOTFiMzQtYzc0Ny00YTk1LWFiMGYtNDM0ZmJkOGYzYjNj',\n",
       " 'ZDY4MThhYjktYWVjOS00NTQ3LTg4NTEtZDIzOTI0ODVhN2Yx',\n",
       " 'ZTFhNmQxNDUtNjAxYS00ZmNhLWJkMGEtZWI4N2FmOWMxZjEw',\n",
       " 'NWMzMWQzOGMtZjQwYS00ZmZkLWFiYmYtMGQ4ODk5ODlmMDk4',\n",
       " 'YjMzZWFmMDItMTg4MC00Nzk3LWIwNWItMzIzMjgyYTZjYjM4',\n",
       " 'MTIyMTBjZmMtOWQzOC00NGE5LThmYzItNjc0ZDk4MmIyYjdm',\n",
       " 'NDQwNTk0YTAtYzA2YS00MWY4LWI3MjUtNWQzNjNmOTcxZjk2',\n",
       " 'OTY0ZjhlZTMtZGYwOS00ODMzLThlOWMtMjc2MDlmZTY4Nzhh',\n",
       " 'Y2FlMjgzODctMjEwZi00MTJkLWI2ZTctOTZkMTE3ZTllNGI3',\n",
       " 'OGI2MGJlZjYtYzI4OC00ZDRjLWFjYjktYjQ4ZmM3MWE3YjE4',\n",
       " 'MmFmNzI5MTYtMzIzNS00YjliLThkYWUtYjNiZDI2MWRjNWNm',\n",
       " 'YTg5ZTY3NzQtZjk4Mi00MGM2LTg1ZTgtMDNkOGY5MWQ1NGM4',\n",
       " 'Y2VjM2QzMDEtMDU4OC00ZDc1LTlkYjItZWEwZTQ5ZjBiOWEz',\n",
       " 'YzU0ZDNjNzUtMGE2Mi00NDdhLTk4N2UtNWQzZTE0ZDdiZGM0',\n",
       " 'YTU5NmNmZjktMGRkNS00NzhkLTgwNWEtNGZjMzQ3ODk1YjRl',\n",
       " 'YTJmZTJhZGYtMjM0Ni00ZTFkLTkzYTYtNDE3NzcyZmI0YzUy',\n",
       " 'MDE0MzFkMGMtNWY2NC00NjkyLWE4ZWUtMmIyZmU5YjAwYmE4',\n",
       " 'ZGIwMGFjNWUtNGZiZC00OTI0LTg0NjMtMzc3OGMwMTU0NTlk',\n",
       " 'YzZmMjg0N2YtYWNhMy00YzhiLWI2MDktOWZiZjY3NTQyMDE0',\n",
       " 'ODRkNjMwZTQtODFhZS00ZjVmLTk0ODgtMTlmZGJiNTFkNWQ3',\n",
       " 'ZTgyYmEzNjAtMjExMi00ZmRhLWI2NjYtZTQ2NzViMmQ5MDU3',\n",
       " 'MWRmZjUwOGYtNjU1NS00ZWE2LTkzYTgtMzQxZGQ4YjY3MDVh',\n",
       " 'MDcyMjQ0ZjgtNTNhYy00ZTZmLThkNTYtZGMwZTAyOThiOGEx',\n",
       " 'ZGRmYzNkZmUtYzY5NC00ZjVmLWEzZjMtYzUzMTQ0ODYxMjkz',\n",
       " 'MTRkN2M0ZDEtODhkNy00ZjgxLTkxMTEtYTUxMzRmYTk4MTU4',\n",
       " 'NzM5MjQ4YWUtNTM4NC00ZTk3LTg1NzYtZjMwZDRlMGFhYzY1',\n",
       " 'ZWUzMzNjMDAtZGNiZS00ZmUxLWEzYzktOTQzY2VjYjliZjkz',\n",
       " 'N2FlMTJkYWQtYjBmMy00MWE1LTk5ZTYtYjk0NjE2MDJiNGU0',\n",
       " 'ZjMwZTQ0MDktNDA5MS00M2Y0LTlhZTgtYWJmMDkwNzAyMTU3',\n",
       " 'NTM0YmQxNjItNTFhZC00NTAwLWJhYTUtYjI0MmUwYjI0NmNj',\n",
       " 'YjRiMzAyYzAtNGVjYS00ODNlLTkzZGYtZjYwZDNiYzNlOGYy',\n",
       " 'ODI2NjExNzYtNWVlNS00ZjI1LThiN2ItNzQ0Yjk2MTA4MWJh',\n",
       " 'NDJmMmE1MGEtODQ0My00ZjMxLThhMzctMWU1MDAyYzMwN2My',\n",
       " 'NjlmMmNlOTQtMDc4Ny00NmNlLWFkOTQtNWRjZTlhNWFlNzQ0']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_metadata = docs[-1].metadata['images']\n",
    "lst_docs = []\n",
    "for doc in child_docs:\n",
    "    figure_indices = find_figure_indices(doc.page_content)\n",
    "    if figure_indices:\n",
    "        for figure_indice in figure_indices:\n",
    "            image = image_metadata[figure_indice]\n",
    "            doc_result = Document(page_content = doc.page_content, metadata={\"header\": json.dumps(doc.metadata), \"source\": \"sam.pdf\", \"image\": image})\n",
    "            lst_docs.append(doc_result)\n",
    "    else:\n",
    "        doc_result = Document(page_content = doc.page_content, metadata={\"header\": json.dumps(doc.metadata), \"source\": \"sam.pdf\", \"image\": None})\n",
    "        lst_docs.append(doc_result)\n",
    "vector_store_multi_modal.add_documents(documents=lst_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90bff248-b474-4a6d-9453-44bfd32236e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:23:55.868866Z",
     "iopub.status.busy": "2024-07-19T13:23:55.867314Z",
     "iopub.status.idle": "2024-07-19T13:24:08.616788Z",
     "shell.execute_reply": "2024-07-19T13:24:08.615248Z",
     "shell.execute_reply.started": "2024-07-19T13:23:55.868866Z"
    }
   },
   "outputs": [],
   "source": [
    "index = await FAISS.afrom_documents(documents=child_docs, embedding=aoai_embeddings)\n",
    "retriever_base = index.as_retriever(search_type=\"similarity\",search_kwargs = {\"k\" : 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8643ba3-ae51-4357-a2e0-cfc1a46e813f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lets do the RAG Now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6216c-d26f-4845-8e8a-ccf1d34fb0ae",
   "metadata": {},
   "source": [
    "### Load the AOAI Chat Class from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "143f0c4a-0fe7-4d2c-8b8b-f1274f5ceba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:21:54.560493Z",
     "iopub.status.busy": "2024-07-19T13:21:54.559972Z",
     "iopub.status.idle": "2024-07-19T13:21:57.992039Z",
     "shell.execute_reply": "2024-07-19T13:21:57.990491Z",
     "shell.execute_reply.started": "2024-07-19T13:21:54.560493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop'}, id='run-3e84a363-fe7e-424b-a41f-6f3a3c33cffa-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],  \n",
    "                      api_version = \"2024-06-01\",\n",
    "                      azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                      model= os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                      streaming=True)\n",
    "llm([HumanMessage(\"Hi\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73033ec9-2458-4002-bb2b-c080cb7cc703",
   "metadata": {},
   "source": [
    "### Multi Modal RAG (Ingestion Side Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35e4c2f9-0727-49bd-95af-c13150223e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:25:08.180177Z",
     "iopub.status.busy": "2024-07-19T13:25:08.179671Z",
     "iopub.status.idle": "2024-07-19T13:25:12.141171Z",
     "shell.execute_reply": "2024-07-19T13:25:12.140107Z",
     "shell.execute_reply.started": "2024-07-19T13:25:08.180177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'Header 1': '1\\\\. Introduction'},\n",
       "  {'Header 1': '3\\\\. Large Language Models'},\n",
       "  {'Header 1': '2\\\\. Background',\n",
       "   'Header 2': '2.10. Pre-Training Objectives',\n",
       "   'Header 3': '2.12. LLMs Adaptation Stages'},\n",
       "  {'Header 1': '3\\\\. Large Language Models',\n",
       "   'Header 2': '3.1. Pre-Trained LLMs'},\n",
       "  {'Header 1': 'Abstract'}],\n",
       " 'answer': '58 papers were released in the year 2021 with LLMs + Alignment.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "def format_docs(docs):\n",
    "    to_return =  \"\\n\\n\".join(str(doc.metadata) + \"\\n\" + doc.page_content for doc in docs)\n",
    "    return to_return\n",
    "    \n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableMap(\n",
    "    {\"documents\": retriever_base, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "rag_chain_with_source.invoke(\"How many papers was released in the year 2021 with LLMs + Alignment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61eea339-2b2f-4332-add7-cdf739f02f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:36:48.303327Z",
     "iopub.status.busy": "2024-07-19T13:36:48.302300Z",
     "iopub.status.idle": "2024-07-19T13:36:51.161209Z",
     "shell.execute_reply": "2024-07-19T13:36:51.159029Z",
     "shell.execute_reply.started": "2024-07-19T13:36:48.303327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'Header 1': '2\\\\. Background',\n",
       "   'Header 2': '2.10. Pre-Training Objectives'},\n",
       "  {'Header 1': '2\\\\. Background',\n",
       "   'Header 2': '2.10. Pre-Training Objectives',\n",
       "   'Header 3': '2.12. LLMs Adaptation Stages',\n",
       "   'Header 4': '2.12.2. Fine-Tuning'},\n",
       "  {'Header 1': '2\\\\. Background',\n",
       "   'Header 2': '2.10. Pre-Training Objectives',\n",
       "   'Header 3': '2.12. LLMs Adaptation Stages',\n",
       "   'Header 4': '2.12.1. Pre-Training'},\n",
       "  {'Header 1': '1\\\\. Introduction'},\n",
       "  {'Header 1': '2\\\\. Background',\n",
       "   'Header 2': '2.10. Pre-Training Objectives'}],\n",
       " 'answer': 'The module present between pre-training and reward modeling is instruction-tuning.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_with_source.invoke(\"Which module is present between pre training and reward modelling?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "882efb51-8166-4e96-a998-1ec4db0d3aab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:38:12.565033Z",
     "iopub.status.busy": "2024-07-19T13:38:12.564502Z",
     "iopub.status.idle": "2024-07-19T13:38:15.783874Z",
     "shell.execute_reply": "2024-07-19T13:38:15.782178Z",
     "shell.execute_reply.started": "2024-07-19T13:38:12.565033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'Header 1': '2\\\\. Background',\n",
       "   'Header 2': '2.10. Pre-Training Objectives'},\n",
       "  {'Header 1': '3\\\\. Large Language Models'},\n",
       "  {'Header 1': '2\\\\. Background', 'Header 2': '2.5. Layer Normalization'},\n",
       "  {'Header 1': '1\\\\. Introduction'},\n",
       "  {'Header 1': '2\\\\. Background'}],\n",
       " 'answer': 'The components of RLHF shown in green dashed lines include the **LLM** and **Reward**.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_with_source.invoke(\"Which component are part of RLHF shown in green dash lines?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4434cf-daa1-4920-8e1f-c038b29dfb17",
   "metadata": {},
   "source": [
    "### Multi Modal RAG (Both Ingestion Side + Calling Side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "793d1810-1bd7-4cbc-bc92-b623138f98d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:41:25.264291Z",
     "iopub.status.busy": "2024-07-19T13:41:25.262725Z",
     "iopub.status.idle": "2024-07-19T13:41:35.665100Z",
     "shell.execute_reply": "2024-07-19T13:41:35.661975Z",
     "shell.execute_reply.started": "2024-07-19T13:41:25.264291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The components part of RLHF (Reinforcement Learning from Human Feedback) shown in green dash lines are the LLM, Reward Modeling, and Reward.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def get_image_text(docs):\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata['image']:\n",
    "            b64_images.append(doc.metadata['image'])\n",
    "        else:\n",
    "            texts.append(doc.page_content)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "retriever_multi_modal = vector_store_multi_modal.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain_multimodal_rag = (\n",
    "    {\n",
    "        \"context\": retriever_multi_modal | RunnableLambda(get_image_text),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(img_prompt_func)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain_multimodal_rag.invoke(\"Which component are part of RLHF shown in green dash lines?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2003f7-4bad-452e-a490-af56d47af7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
